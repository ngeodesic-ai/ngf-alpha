{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7e0c301-6deb-4fb4-92b9-415e3fa4b45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PCA] samples=30 feat=768 cap=30 -> n=8 (cum var=0.9968, whiten=True)\n",
      "[Anchors] order: ['flip_h', 'flip_v', 'rotate']\n",
      "[01] true=rotate  pred=flip_v  ok=False dist*=9.185 conf=0.863 probs=[1.375e-01 8.625e-01 2.914e-10]\n",
      "[02] true=rotate  pred=flip_v  ok=False dist*=10.834 conf=0.846 probs=[1.540e-01 8.460e-01 3.667e-10]\n",
      "[03] true=rotate  pred=flip_h  ok=False dist*=7.783 conf=0.835 probs=[8.347e-01 1.652e-01 1.519e-04]\n",
      "[04] true=rotate  pred=rotate  ok=True dist*=4.239 conf=0.999 probs=[7.448e-05 6.429e-04 9.993e-01]\n",
      "[05] true=flip_h  pred=flip_v  ok=False dist*=9.185 conf=0.863 probs=[1.375e-01 8.625e-01 2.914e-10]\n",
      "[06] true=flip_h  pred=flip_v  ok=False dist*=10.834 conf=0.846 probs=[1.540e-01 8.460e-01 3.667e-10]\n",
      "[07] true=flip_h  pred=flip_h  ok=True dist*=7.783 conf=0.835 probs=[8.347e-01 1.652e-01 1.519e-04]\n",
      "[08] true=flip_h  pred=rotate  ok=False dist*=4.239 conf=0.999 probs=[7.448e-05 6.429e-04 9.993e-01]\n",
      "[09] true=flip_v  pred=flip_v  ok=True dist*=9.185 conf=0.863 probs=[1.375e-01 8.625e-01 2.914e-10]\n",
      "[10] true=flip_v  pred=flip_v  ok=True dist*=10.834 conf=0.846 probs=[1.540e-01 8.460e-01 3.667e-10]\n",
      "[11] true=flip_v  pred=flip_h  ok=False dist*=7.783 conf=0.835 probs=[8.347e-01 1.652e-01 1.519e-04]\n",
      "[12] true=flip_v  pred=rotate  ok=False dist*=4.239 conf=0.999 probs=[7.448e-05 6.429e-04 9.993e-01]\n",
      "\n",
      "[ARC-12] Accuracy: 33.3% | Mean confidence: 0.886 | Mode=geodesic\n",
      "[Sanity] anchor order: ['flip_h', 'flip_v', 'rotate']\n",
      "[Sanity] distances: [5.731 5.818 5.631]\n",
      "[Sanity] chosen: rotate\n"
     ]
    }
   ],
   "source": [
    "# step9_hybrid_geodesic.py\n",
    "# CPU-only, NumPy + transformers. Precision Step 9 (geodesics), optional tiny damping.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy.linalg import inv\n",
    "\n",
    "\n",
    "# ---------- Config ----------\n",
    "REDUCED_VAR = 0.99\n",
    "LAMBDA = 0.45          # give a bit more curvature\n",
    "DT = 0.03\n",
    "STEPS = 800\n",
    "GAMMA = 0.02           # tiny damping\n",
    "SEED = 42\n",
    "MODE = \"geodesic\"\n",
    "EPS = 1e-6             # used in potential/grad\n",
    "\n",
    "\n",
    "# --- PCA ---\n",
    "REDUCED_VAR = 0.99\n",
    "MIN_DIMS = 8\n",
    "MAX_DIMS = 16\n",
    "\n",
    "\n",
    "# 1) Drop this list where you define training data\n",
    "train_prompts = [\n",
    "    # ---- rotate (10) ----\n",
    "    \"Identify the pattern: Input grid [[1,2],[3,4]] -> Output [[3,1],[4,2]] (90° cw). Apply the same idea.\",\n",
    "    \"Perform a 90° clockwise rotation: [[5,6],[7,8]] → [[7,5],[8,6]]. Generalize this transformation.\",\n",
    "    \"Rotate right by 90 degrees: [[2,1],[4,3]] → [[4,2],[3,1]]. Use this rotation behavior.\",\n",
    "    \"Turn the grid 90° to the right: [[0,1],[2,3]] → [[2,0],[3,1]]. Maintain this mapping rule.\",\n",
    "    \"Apply a quarter‑turn clockwise: [[9,8],[7,6]] → [[7,9],[6,8]]. Keep consistent with 90° cw.\",\n",
    "    \"Use a 90° cw rotation on a 3×3: [[1,2,3],[4,5,6],[7,8,9]] → [[7,4,1],[8,5,2],[9,6,3]].\",\n",
    "    \"Clockwise quarter‑turn: [[3,0],[0,1]] → [[0,3],[1,0]]. Follow the same rotation rule.\",\n",
    "    \"Rotate 90° cw: [[1,0],[0,2]] → [[0,1],[2,0]]. Preserve clockwise orientation.\",\n",
    "    \"Quarter‑turn right: [[2,0],[5,7]] → [[5,2],[7,0]]. Use standard 90° cw mapping.\",\n",
    "    \"3×3 right rotation: [[0,2,0],[1,0,1],[0,2,0]] → [[0,1,0],[2,0,2],[0,1,0]].\",\n",
    "\n",
    "    # ---- flip_h (10) ----\n",
    "    \"Flip horizontally: [[1,2],[3,4]] → [[2,1],[4,3]]. Mirror columns.\",\n",
    "    \"Reflect left‑right: [[5,6],[7,8]] → [[6,5],[8,7]]. Keep rows, swap columns.\",\n",
    "    \"Horizontal mirror: [[0,1],[2,3]] → [[1,0],[3,2]]. Apply left↔right reflection.\",\n",
    "    \"Left‑right flip on 3×3: [[1,2,3],[4,5,6],[7,8,9]] → [[3,2,1],[6,5,4],[9,8,7]].\",\n",
    "    \"Mirror columns: [[2,1],[4,3]] → [[1,2],[3,4]]. Keep the same flip rule.\",\n",
    "    \"Flip horizontally: [[0,2],[5,7]] → [[2,0],[7,5]]. Swap each row’s ends.\",\n",
    "    \"Reflect across vertical axis: [[3,0],[0,1]] → [[0,3],[1,0]]. Preserve row order.\",\n",
    "    \"Left↔right reflect: [[1,0],[0,2]] → [[0,1],[2,0]]. Symmetric across the center line.\",\n",
    "    \"Horizontal flip (3×3): [[0,2,0],[1,0,1],[0,2,0]] → [[0,2,0],[1,0,1],[0,2,0]].\",\n",
    "    \"Mirror columns: [[9,8],[7,6]] → [[8,9],[6,7]].\",\n",
    "\n",
    "    # ---- flip_v (10) ----\n",
    "    \"Flip vertically: [[1,2],[3,4]] → [[3,4],[1,2]]. Mirror rows.\",\n",
    "    \"Reflect top‑bottom: [[5,6],[7,8]] → [[7,8],[5,6]]. Keep columns, swap rows.\",\n",
    "    \"Vertical mirror: [[0,1],[2,3]] → [[2,3],[0,1]]. Apply top↔bottom reflection.\",\n",
    "    \"Top‑bottom flip on 3×3: [[1,2,3],[4,5,6],[7,8,9]] → [[7,8,9],[4,5,6],[1,2,3]].\",\n",
    "    \"Mirror rows: [[2,1],[4,3]] → [[4,3],[2,1]]. Standard vertical flip.\",\n",
    "    \"Flip vertically: [[0,2],[5,7]] → [[5,7],[0,2]]. Swap row order.\",\n",
    "    \"Reflect across horizontal axis: [[3,0],[0,1]] → [[0,1],[3,0]]. Keep columns fixed.\",\n",
    "    \"Top↔bottom reflect: [[1,0],[0,2]] → [[0,2],[1,0]]. Maintain column structure.\",\n",
    "    \"Vertical flip (3×3): [[0,2,0],[1,0,1],[0,2,0]] → [[0,2,0],[1,0,1],[0,2,0]].\",\n",
    "    \"Mirror rows: [[9,8],[7,6]] → [[7,6],[9,8]].\",\n",
    "]\n",
    "\n",
    "labels = (\n",
    "    [\"rotate\"]*10 +\n",
    "    [\"flip_h\"]*10 +\n",
    "    [\"flip_v\"]*10\n",
    ")\n",
    "\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ---------- Latent extraction ----------\n",
    "_device = torch.device(\"cpu\")\n",
    "\n",
    "_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(_device)\n",
    "_model.eval()\n",
    "\n",
    "def get_latent(prompt: str) -> np.ndarray:\n",
    "    \"\"\"Mean-pool last hidden state as latent.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        toks = _tokenizer(prompt, return_tensors=\"pt\").to(_device)\n",
    "        out = _model(**toks, output_hidden_states=True)\n",
    "        hs = out.hidden_states[-1][0].cpu().numpy()  # (seq, hidden)\n",
    "    return hs.mean(axis=0)  # (hidden,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pca_fit_transform(vectors, target_var=0.95, MIN_DIMS=2, MAX_DIMS=8, verbose=True):\n",
    "    \"\"\"\n",
    "    vectors: (n_samples, n_features)\n",
    "    Chooses n_components safely based on available samples/features and target variance.\n",
    "    \"\"\"\n",
    "    vectors = np.asarray(vectors)\n",
    "    n_samples, n_features = vectors.shape\n",
    "    n_cap = min(n_samples, n_features)          # hard cap from sklearn\n",
    "    if n_cap <= 0:\n",
    "        raise ValueError(\"Empty training set for PCA.\")\n",
    "    \n",
    "    # First fit with full dimensionality up to the cap to measure variance captured.\n",
    "    probe_n = min(n_cap, MAX_DIMS)              # don't waste time probing huge dims\n",
    "    pca_probe = PCA(n_components=probe_n, whiten=False, svd_solver=\"full\").fit(vectors)\n",
    "    cumvar = np.cumsum(pca_probe.explained_variance_ratio_)\n",
    "    # Smallest k meeting target variance (or probe_n if target not reached)\n",
    "    k = int(np.searchsorted(cumvar, target_var) + 1)\n",
    "    \n",
    "    # Final n: respect MIN/MAX and the cap\n",
    "    n = max(MIN_DIMS, min(MAX_DIMS, k))\n",
    "    n = min(n, n_cap)\n",
    "    \n",
    "    # If we only have 1–2 samples/features, we may be forced to 1D.\n",
    "    if n < MIN_DIMS and n_cap >= 1:\n",
    "        # fall back gracefully; keep whiten=False when n is tiny\n",
    "        n = n_cap\n",
    "    \n",
    "    whiten = (n >= 2)   # avoid whitening in degenerate 1D cases\n",
    "    pca = PCA(n_components=n, whiten=whiten, svd_solver=\"full\").fit(vectors)\n",
    "    Z = pca.transform(vectors)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[PCA] samples={n_samples} feat={n_features} cap={n_cap} \"\n",
    "              f\"-> n={pca.n_components_} (cum var={cumvar[min(n-1, len(cumvar)-1)]:.4f}, \"\n",
    "              f\"whiten={whiten})\")\n",
    "        if pca.n_components_ < 2:\n",
    "            print(\"NOTE: PCA ended up 1D due to limited samples/features. \"\n",
    "                  \"Add more training prompts or lower MIN_DIMS if downstream expects ≥2D.\")\n",
    "    return pca, Z\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Conformally flat metric from multi-mass potential ----------\n",
    "def potential(x: np.ndarray, centers: np.ndarray, masses: np.ndarray) -> float:\n",
    "    # V(x) = - sum_i M_i / (||x - c_i|| + eps)\n",
    "    diffs = x[None, :] - centers\n",
    "    dists = np.linalg.norm(diffs, axis=1) + EPS\n",
    "    return -np.sum(masses / dists)\n",
    "\n",
    "def grad_potential(x: np.ndarray, centers: np.ndarray, masses: np.ndarray) -> np.ndarray:\n",
    "    # ∇V = - sum_i M_i * (-(x - c_i)) / (||x - c_i||^3 + eps)\n",
    "    diffs = x[None, :] - centers  # (k,d)\n",
    "    dists = np.linalg.norm(diffs, axis=1) + EPS  # (k,)\n",
    "    # d/dx (1/r) = - (x-c)/r^3\n",
    "    terms = masses[:, None] * diffs / (dists**3)[:, None]  # (k,d)\n",
    "    return -np.sum(terms, axis=0)\n",
    "\n",
    "def lnphi_and_grad(x: np.ndarray, centers: np.ndarray, masses: np.ndarray):\n",
    "    V = potential(x, centers, masses)\n",
    "    lnphi = LAMBDA * V\n",
    "    # ∇lnφ = λ ∇V\n",
    "    g_lnphi = LAMBDA * grad_potential(x, centers, masses)\n",
    "    return lnphi, g_lnphi\n",
    "\n",
    "def christoffel_conformal(x: np.ndarray, v: np.ndarray, centers: np.ndarray, masses: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Γ^i_{jk} v^j v^k for g_ij = φ^2 δ_ij with φ = exp(λV).\n",
    "    Γ^i_{jk} = δ^i_j ∂_k lnφ + δ^i_k ∂_j lnφ - δ_jk ∂^i lnφ\n",
    "    Contract with v^j v^k: 2 v_i (v·∇lnφ) - ||v||^2 ∂^i lnφ\n",
    "    Returned as vector with upper index i.\n",
    "    \"\"\"\n",
    "    _, grad_lnphi = lnphi_and_grad(x, centers, masses)\n",
    "    v_dot = float(np.dot(v, grad_lnphi))\n",
    "    v_norm2 = float(np.dot(v, v))\n",
    "    return 2.0 * v * v_dot - v_norm2 * grad_lnphi\n",
    "\n",
    "def rk4_step(x: np.ndarray, v: np.ndarray, dt: float, centers: np.ndarray, masses: np.ndarray):\n",
    "    \"\"\"\n",
    "    Geodesic ODE in coordinates (x, v):\n",
    "      dx/dτ = v\n",
    "      dv/dτ = - Γ(x)[v,v] - γ v\n",
    "    \"\"\"\n",
    "    def a(x_, v_):\n",
    "        return -christoffel_conformal(x_, v_, centers, masses) - GAMMA * v_\n",
    "\n",
    "    k1x = v\n",
    "    k1v = a(x, v)\n",
    "\n",
    "    k2x = v + 0.5 * dt * k1v\n",
    "    k2v = a(x + 0.5 * dt * k1x, v + 0.5 * dt * k1v)\n",
    "\n",
    "    k3x = v + 0.5 * dt * k2v\n",
    "    k3v = a(x + 0.5 * dt * k2x, v + 0.5 * dt * k2v)\n",
    "\n",
    "    k4x = v + dt * k3v\n",
    "    k4v = a(x + dt * k3x, v + dt * k3v)\n",
    "\n",
    "    x_new = x + (dt / 6.0) * (k1x + 2*k2x + 2*k3x + k4x)\n",
    "    v_new = v + (dt / 6.0) * (k1v + 2*k2v + 2*k3v + k4v)\n",
    "    return x_new, v_new\n",
    "\n",
    "def integrate_geodesic(x0: np.ndarray, v0: np.ndarray, centers: np.ndarray, masses: np.ndarray,\n",
    "                       steps: int = STEPS, dt: float = DT):\n",
    "    x, v = x0.copy(), v0.copy()\n",
    "    traj = [x.copy()]\n",
    "    for _ in range(steps):\n",
    "        x, v = rk4_step(x, v, dt, centers, masses)\n",
    "        traj.append(x.copy())\n",
    "    return np.array(traj)\n",
    "\n",
    "def class_stats(Z, labels, uniq):\n",
    "    stats = {}\n",
    "    for u in uniq:\n",
    "        idxs = [i for i,l in enumerate(labels) if l == u]\n",
    "        G = Z[idxs]\n",
    "        mu = G.mean(axis=0)\n",
    "        # regularized covariance (diagonal if tiny sample)\n",
    "        C = np.cov(G.T) if G.shape[0] > G.shape[1] else np.diag(np.var(G, axis=0) + 1e-6)\n",
    "        # ridge for stability\n",
    "        C = C + 1e-4 * np.eye(C.shape[0])\n",
    "        stats[u] = {\"mu\": mu, \"invC\": inv(C)}\n",
    "    return stats\n",
    "\n",
    "def mahalanobis_distances(z, stats, order):\n",
    "    ds = []\n",
    "    for u in order:\n",
    "        mu, invC = stats[u][\"mu\"], stats[u][\"invC\"]\n",
    "        d = z - mu\n",
    "        ds.append(float(np.sqrt(d @ invC @ d)))\n",
    "    return np.array(ds)\n",
    "\n",
    "# Use this instead of Euclidean in classify_endpoint:\n",
    "def classify_endpoint_maha(z_final_red, stats, order):\n",
    "    d = mahalanobis_distances(z_final_red, stats, order)\n",
    "    j = int(np.argmin(d))\n",
    "    return j, float(d[j]), d\n",
    "\n",
    "# --- Compute anchor centers and semantic masses ---\n",
    "def compute_anchors(Z, labels):\n",
    "    uniq = sorted(set(labels))  # actual order used to build centers\n",
    "    centers, masses = [], []\n",
    "    for u in uniq:\n",
    "        idxs = [i for i,l in enumerate(labels) if l == u]\n",
    "        group = Z[idxs]\n",
    "        c = group.mean(axis=0)\n",
    "        centers.append(c)\n",
    "        spread = float(np.mean(np.linalg.norm(group - c, axis=1)) + 1e-6)\n",
    "        masses.append(1.0 / spread)  # tighter cluster => heavier mass\n",
    "    centers = np.stack(centers, axis=0)\n",
    "    masses  = np.array(masses, dtype=float)\n",
    "    masses /= (masses.sum() + 1e-9)\n",
    "    return centers, masses, uniq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Example end-to-end ----------\n",
    "def solve_with_geodesics(prompt: str,\n",
    "                         pca: PCA,\n",
    "                         anchor_centers_red: np.ndarray,\n",
    "                         masses: np.ndarray):\n",
    "    z_full = get_latent(prompt)                 # (hidden,)\n",
    "    z_red  = pca.transform(z_full[None, :])[0]  # (d,)\n",
    "\n",
    "    # Start slightly displaced with small initial velocity toward negative grad V\n",
    "    lnphi, g_lnphi = lnphi_and_grad(z_red, anchor_centers_red, masses)\n",
    "    v0 = -0.1 * g_lnphi / (np.linalg.norm(g_lnphi) + 1e-9)\n",
    "\n",
    "    traj = integrate_geodesic(z_red, v0, anchor_centers_red, masses, steps=STEPS, dt=DT)\n",
    "    z_final_red = traj[-1]\n",
    "    # Map back to full space (optional): x_full ≈ z_red * P^T + mean\n",
    "    z_full_final = pca.inverse_transform(z_final_red[None, :])[0]\n",
    "    return z_full_final, traj\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# ARC warp-interference solver\n",
    "# ----------------------------\n",
    "\n",
    "# 1) Minimal ARC transforms we’ll support right now\n",
    "def rot90_cw(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.rot90(g, k=3).tolist()\n",
    "\n",
    "def rot180(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.rot90(g, k=2).tolist()\n",
    "\n",
    "def rot270_cw(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.rot90(g, k=1).tolist()\n",
    "\n",
    "def flip_h(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.flip(g, axis=1).tolist()\n",
    "\n",
    "def flip_v(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.flip(g, axis=0).tolist()\n",
    "\n",
    "# Map anchor index -> transform; adjust to your labels/anchor order\n",
    "TRANSFORM_BY_LABEL = {\n",
    "    \"rotate\": rot90_cw,       # label 0\n",
    "    \"flip_h\": flip_h,         # label 1\n",
    "    \"flip_v\": flip_v,         # label 2\n",
    "    # add more labels/anchors as you train them:\n",
    "    # \"rot180\": rot180,\n",
    "    # \"rot270\": rot270_cw,\n",
    "}\n",
    "\n",
    "# If you created anchors via sorted(set(labels)), keep the same order here:\n",
    "ANCHOR_LABELS_IN_ORDER = [\"rotate\", \"flip_h\", \"flip_v\"]  # <- keep in sync with your training labels\n",
    "\n",
    "def grid_to_prompt(grid):\n",
    "    # simple text promptization so GPT-2 latent “sees” the exact instance\n",
    "    return f\"Identify the pattern: Input grid {grid} -> Output ? (choose rotate 90° cw, flip_h, or flip_v).\"\n",
    "\n",
    "def classify_endpoint(z_final_red, centers):\n",
    "    d = np.linalg.norm(centers - z_final_red, axis=1)\n",
    "    j = int(np.argmin(d))\n",
    "    return j, float(d[j]), d\n",
    "\n",
    "def arc_latent_for_grid(grid, pca):\n",
    "    prompt = grid_to_prompt(grid)\n",
    "    z_full = get_latent(prompt)\n",
    "    z_red = pca.transform(z_full.reshape(1, -1))[0]\n",
    "    return z_red\n",
    "\n",
    "def run_warp_interference(z_red, centers, masses, steps, dt, mode=\"geodesic\"):\n",
    "    # build initial velocity along −∇lnφ (fallback to small random)\n",
    "    _, g_lnphi = lnphi_and_grad(z_red, centers, masses)\n",
    "    g_norm = float(np.linalg.norm(g_lnphi))\n",
    "    if g_norm < 1e-8:\n",
    "        rng = np.random.default_rng(0)\n",
    "        v0 = rng.normal(size=z_red.shape).astype(float)\n",
    "        v0 /= (np.linalg.norm(v0) + 1e-9)\n",
    "        v0 *= 0.05\n",
    "    else:\n",
    "        v0 = -g_lnphi / (g_norm + 1e-9) * 0.1\n",
    "\n",
    "    if mode == \"geodesic\":\n",
    "        traj = integrate_geodesic(x0=z_red, v0=v0, centers=centers, masses=masses, steps=steps, dt=dt)\n",
    "        z_final = traj[-1]\n",
    "    else:\n",
    "        # Stage-9 linearized nudge (if you kept integrate_nudge from earlier)\n",
    "        traj = integrate_nudge(z_red, centers, masses, steps=350, dt=0.05, k=2.0, gamma=0.2)\n",
    "        z_final = traj[-1]\n",
    "\n",
    "    return z_final\n",
    "\n",
    "def solve_arc_task(input_grid, *, verbose=True):\n",
    "    \"\"\"\n",
    "    input_grid: e.g. [[1,2],[3,4]] or a 3x3 integer grid.\n",
    "    Returns: predicted_output_grid (same shape as input)\n",
    "    \"\"\"\n",
    "    # 1) embed this specific instance\n",
    "    z_red = arc_latent_for_grid(input_grid, pca)\n",
    "\n",
    "    # 2) warp-interference geodesic to pick the anchor\n",
    "    z_final = run_warp_interference(z_red, anchor_centers_red, masses, steps=STEPS, dt=DT, mode=MODE)\n",
    "\n",
    "    # 3) nearest anchor = chosen transform\n",
    "    # j, dmin, all_d = classify_endpoint(z_final, anchor_centers_red)\n",
    "    # chosen_label = ANCHOR_LABELS_IN_ORDER[j]\n",
    "\n",
    "    j, dmin, all_d = classify_maha(z_final)\n",
    "    chosen_label = ANCHOR_LABELS_IN_ORDER[j]\n",
    "    \n",
    "    transform = TRANSFORM_BY_LABEL.get(chosen_label, None)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[ARC] Endpoint nearest anchor: {j} ({chosen_label}) at distance {dmin:.4f}\")\n",
    "        print(f\"[ARC] Distances to anchors:\", np.array_str(all_d, precision=4, suppress_small=True))\n",
    "\n",
    "    if transform is None:\n",
    "        raise ValueError(f\"No transform mapped for anchor label '{chosen_label}'\")\n",
    "\n",
    "    # 4) apply transform to the grid\n",
    "    output_grid = transform(input_grid)\n",
    "    return output_grid\n",
    "\n",
    "def classify_maha(z_red):\n",
    "    d = mahalanobis_distances(z_red, cls_stats, ANCHOR_LABELS_IN_ORDER)\n",
    "    j = int(np.argmin(d))\n",
    "    return j, float(d[j]), d\n",
    "\n",
    "def anchor_confidence(distances, tau=None):\n",
    "    distances = np.asarray(distances, dtype=float)\n",
    "    # temperature from scale of non-min distances to avoid saturation\n",
    "    diffs = distances - distances.min()\n",
    "    if tau is None:\n",
    "        # robust scale: median of diffs (fallback to 1.0)\n",
    "        tau = float(np.median(diffs[diffs > 1e-9])) if np.any(diffs > 1e-9) else 1.0\n",
    "        tau = max(tau, 0.5)  # floor\n",
    "    s = np.exp(-diffs / tau)\n",
    "    p = s / (s.sum() + 1e-9)\n",
    "    j = int(np.argmin(distances))\n",
    "    return j, float(p[j]), p\n",
    "\n",
    "\n",
    "# Build 12 toy cases covering rotate / flip_h / flip_v evenly\n",
    "def make_cases():\n",
    "    # small 2x2 and 3x3 to vary structure\n",
    "    cases = [\n",
    "        # rotate 90° cw\n",
    "        {\"grid\": [[1,2],[3,4]], \"label\":\"rotate\"},\n",
    "        {\"grid\": [[5,6],[7,8]], \"label\":\"rotate\"},\n",
    "        {\"grid\": [[1,0,2],[0,1,0],[2,0,1]], \"label\":\"rotate\"},\n",
    "        {\"grid\": [[9,8,7],[6,5,4],[3,2,1]], \"label\":\"rotate\"},\n",
    "        # flip_h\n",
    "        {\"grid\": [[1,2],[3,4]], \"label\":\"flip_h\"},\n",
    "        {\"grid\": [[5,6],[7,8]], \"label\":\"flip_h\"},\n",
    "        {\"grid\": [[1,0,2],[0,1,0],[2,0,1]], \"label\":\"flip_h\"},\n",
    "        {\"grid\": [[9,8,7],[6,5,4],[3,2,1]], \"label\":\"flip_h\"},\n",
    "        # flip_v\n",
    "        {\"grid\": [[1,2],[3,4]], \"label\":\"flip_v\"},\n",
    "        {\"grid\": [[5,6],[7,8]], \"label\":\"flip_v\"},\n",
    "        {\"grid\": [[1,0,2],[0,1,0],[2,0,1]], \"label\":\"flip_v\"},\n",
    "        {\"grid\": [[9,8,7],[6,5,4],[3,2,1]], \"label\":\"flip_v\"},\n",
    "    ]\n",
    "    # attach ground-truth transformed grids using your transform map\n",
    "    out = []\n",
    "    for c in cases:\n",
    "        tfunc = TRANSFORM_BY_LABEL[c[\"label\"]]\n",
    "        out.append({\"grid\": c[\"grid\"], \"label\": c[\"label\"], \"target\": tfunc(c[\"grid\"])})\n",
    "    return out\n",
    "\n",
    "def run_arc_benchmark_12(verbose=True, tau=None):\n",
    "    cases = make_cases()\n",
    "    correct, confs = 0, []\n",
    "    for idx, case in enumerate(cases, 1):\n",
    "        z_red = arc_latent_for_grid(case[\"grid\"], pca)\n",
    "        z_final = run_warp_interference(z_red, anchor_centers_red, masses, steps=STEPS, dt=DT, mode=MODE)\n",
    "\n",
    "        # **Mahalanobis-based decision and calibrated confidence**\n",
    "        j, dmin, dists = classify_endpoint_maha(z_final, cls_stats, ANCHOR_LABELS_IN_ORDER)\n",
    "        j_hat, conf, probs = anchor_confidence(dists, tau=tau)\n",
    "\n",
    "        pred_label = ANCHOR_LABELS_IN_ORDER[j_hat]\n",
    "        pred_grid  = TRANSFORM_BY_LABEL[pred_label](case[\"grid\"])\n",
    "        ok = (pred_label == case[\"label\"]) and (pred_grid == case[\"target\"])\n",
    "        correct += int(ok); confs.append(conf)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[{idx:02d}] true={case['label']:7s} pred={pred_label:7s} \"\n",
    "                  f\"ok={ok} dist*={dmin:.3f} conf={conf:.3f} probs={np.array_str(probs, precision=3)}\")\n",
    "\n",
    "    acc = correct / len(cases)\n",
    "    mean_conf = float(np.mean(confs)) if confs else 0.0\n",
    "    print(f\"\\n[ARC-12] Accuracy: {acc*100:.1f}% | Mean confidence: {mean_conf:.3f} | Mode={MODE}\")\n",
    "    return {\"accuracy\": acc, \"mean_confidence\": mean_conf}\n",
    "\n",
    "\n",
    "# ----- run it -----\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Build training latents and PCA first\n",
    "    train_latents = np.stack([get_latent(p) for p in train_prompts], axis=0)\n",
    "    pca, Z = pca_fit_transform(train_latents, target_var=0.99, MIN_DIMS=8, MAX_DIMS=16)\n",
    "\n",
    "    # 2) Anchors + masses (and record the actual order they were built in)\n",
    "    anchor_centers_red, masses, ANCHOR_LABELS_IN_ORDER = compute_anchors(Z, labels)\n",
    "    masses = masses / (masses.sum() + 1e-9)\n",
    "    print(\"[Anchors] order:\", ANCHOR_LABELS_IN_ORDER)\n",
    "\n",
    "    # 3) Optional: Mahalanobis classifier stats\n",
    "    cls_stats = class_stats(Z, labels, ANCHOR_LABELS_IN_ORDER)\n",
    "\n",
    "    # 4) Run mini-benchmark (uses global pca/anchors/masses/MODE)\n",
    "    _ = run_arc_benchmark_12(verbose=True, tau=1.0)\n",
    "\n",
    "    # 5) Optional sanity: one demo grid end-to-end\n",
    "    demo = [[1,2],[3,4]]\n",
    "    z0 = arc_latent_for_grid(demo, pca)\n",
    "    zf = run_warp_interference(z0, anchor_centers_red, masses, steps=STEPS, dt=DT, mode=MODE)\n",
    "    j, dmin, all_d = classify_endpoint(zf, anchor_centers_red)\n",
    "    pred_label = ANCHOR_LABELS_IN_ORDER[j]\n",
    "    print(\"[Sanity] anchor order:\", ANCHOR_LABELS_IN_ORDER)\n",
    "    print(\"[Sanity] distances:\", np.array_str(all_d, precision=3))\n",
    "    print(\"[Sanity] chosen:\", pred_label)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565e28d-e937-4cf5-b2e9-6164d809805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
