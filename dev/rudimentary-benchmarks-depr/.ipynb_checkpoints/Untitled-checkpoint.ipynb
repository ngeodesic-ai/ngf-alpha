{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8985205-932f-4bc0-b95e-c59e5fb7e886",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (1234463339.py, line 359)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 359\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\\\"\\\\n=== {tag} Summary ===\\\")\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# arc-benchmark-colab-opt-cpu.py\n",
    "# CPU-optimized A/B benchmark for synthetic ARC-style op classification.\n",
    "# - 7-class (A..G) fast head: avoids full-vocab softmax\n",
    "# - Vectorized nudging and calibration through the 7-way path\n",
    "# - Optional smaller model (distilgpt2) for faster CPU inference\n",
    "# - Same external API: make_tasks, run_pass, main()\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # force CPU\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict\n",
    "from dataclasses import dataclass\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "try:\n",
    "    from transformers import DynamicCache\n",
    "except Exception:\n",
    "    DynamicCache = None\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# -----------------------------\n",
    "# Config / Repro\n",
    "# -----------------------------\n",
    "SEED = 43\n",
    "# Smaller model is much faster on CPU; switch back to \"gpt2\" if desired.\n",
    "MODEL_NAME = \"distilgpt2\"\n",
    "N_TASKS = 20  # bump as needed\n",
    "\n",
    "USE_PREFILTER = False\n",
    "USE_CALIBRATION = True\n",
    "CAL_K = 4\n",
    "PRINT_TASKS = True\n",
    "\n",
    "# Warped hyperparams\n",
    "ALPHA_WARPED = 0.90\n",
    "SYMB_STEPS = 12\n",
    "DT = 0.06\n",
    "PULL_STRENGTH = 1.6\n",
    "GAMMA = 0.3\n",
    "LAMBDA_REP = 0.5  # global (precomputed) repulsion\n",
    "\n",
    "# Use last layer for simplicity\n",
    "NUDGE_LAYER = -1\n",
    "\n",
    "# --- CPU knobs ---\n",
    "CPU_FAST = True         # 7-class scoring fast-path\n",
    "TORCH_THREADS = 2       # try 1–4 depending on CPU cores\n",
    "torch.set_num_threads(TORCH_THREADS)\n",
    "torch.set_num_interop_threads(max(1, TORCH_THREADS // 2))\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Ops & letters\n",
    "# -----------------------------\n",
    "OPS = [\"rotate90\",\"flip_h\",\"flip_v\",\"scale2\",\"rotate_then_flip\",\"swap_minmax\",\"shift_down\"]\n",
    "LETTER_MAP = {\n",
    "    \"A\": \"rotate90\",\n",
    "    \"B\": \"flip_h\",\n",
    "    \"C\": \"flip_v\",\n",
    "    \"D\": \"scale2\",\n",
    "    \"E\": \"rotate_then_flip\",\n",
    "    \"F\": \"swap_minmax\",\n",
    "    \"G\": \"shift_down\",\n",
    "}\n",
    "LETTERS = list(LETTER_MAP.keys())\n",
    "\n",
    "# -----------------------------\n",
    "# Model\n",
    "# -----------------------------\n",
    "device = \"cpu\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -----------------------------\n",
    "# Deterministic executors\n",
    "# -----------------------------\n",
    "def rotate90(grid): return [list(col) for col in zip(*grid[::-1])]\n",
    "def flip_h(grid):   return [row[::-1] for row in grid]\n",
    "def flip_v(grid):   return grid[::-1]\n",
    "def scale2(grid):   return [[v*2 for v in row] for row in grid]\n",
    "def rotate_then_flip(grid): return flip_h(rotate90(grid))\n",
    "def swap_minmax(grid):\n",
    "    flat = [v for row in grid for v in row]\n",
    "    mn, mx = min(flat), max(flat)\n",
    "    return [[(mx if v==mn else mn) if v in (mn,mx) else v for v in row] for row in grid]\n",
    "def shift_down(grid): return [grid[-1]] + grid[:-1] if grid else grid\n",
    "\n",
    "EXECUTOR = {\n",
    "    \"rotate90\": rotate90, \"flip_h\": flip_h, \"flip_v\": flip_v, \"scale2\": scale2,\n",
    "    \"rotate_then_flip\": rotate_then_flip, \"swap_minmax\": swap_minmax, \"shift_down\": shift_down,\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Few-shot (letters)\n",
    "# -----------------------------\n",
    "FEWSHOT = (\n",
    "    \"You are given an input grid and an output grid.\\n\"\n",
    "    \"Pick exactly ONE letter from this legend that maps input->output:\\n\"\n",
    "    \"{A: rotate90, B: flip_h, C: flip_v, D: scale2, E: rotate_then_flip, F: swap_minmax, G: shift_down}.\\n\"\n",
    "    \"Return ONLY the letter.\\n\\n\"\n",
    "    \"Input: [[1,9],[2,3]]\\nOutput: [[9,1],[2,3]]\\nAnswer: F\\n\\n\"\n",
    "    \"Input: [[1,2],[3,4]]\\nOutput: [[1,3],[2,4]]\\nAnswer: E\\n\\n\"\n",
    "    \"Input: [[1,2,3],[4,5,6],[7,8,9]]\\nOutput: [[7,8,9],[4,5,6],[1,2,3]]\\nAnswer: C\\n\\n\"\n",
    "    \"Input: [[1,2,3],[4,5,6]]\\nOutput: [[4,5,6],[1,2,3]]\\nAnswer: G\\n\\n\"\n",
    "    \"Input: [[1,2,3],[4,5,6],[7,8,9]]\\nOutput: [[7,4,1],[8,5,2],[9,6,3]]\\nAnswer: A\\n\\n\"\n",
    "    \"Input: [[5,6],[7,8]]\\nOutput: [[6,5],[8,7]]\\nAnswer: B\\n\\n\"\n",
    "    \"Input: [[2,3,4],[5,6,7],[8,9,1]]\\nOutput: [[4,6,8],[10,12,14],[16,18,2]]\\nAnswer: D\\n\"\n",
    ")\n",
    "\n",
    "assert rotate_then_flip([[1,2],[3,4]]) == [[1,3],[2,4]]\n",
    "assert flip_v([[1,2],[3,4],[5,6]]) == [[5,6],[3,4],[1,2]]\n",
    "assert shift_down([[1,2,3],[4,5,6]]) == [[4,5,6],[1,2,3]]\n",
    "\n",
    "# -----------------------------\n",
    "# Tiny PCA on letter embeddings\n",
    "# -----------------------------\n",
    "def last_hidden(text: str, layer: int) -> np.ndarray:\n",
    "    with torch.no_grad(), torch.inference_mode():\n",
    "        out = model(**tokenizer(text, return_tensors=\"pt\").to(device),\n",
    "                    output_hidden_states=True)\n",
    "    return out.hidden_states[layer][:, -1, :].squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "LETTER_PHRASES = [\"{}\", \"Answer: {}\"]  # 2 phrasings only (fast)\n",
    "\n",
    "# Build a small dataset of hidden vectors (letters × phrasings)\n",
    "H = []\n",
    "for L in LETTERS:\n",
    "    for tpl in LETTER_PHRASES:\n",
    "        H.append(last_hidden(tpl.format(L), NUDGE_LAYER))\n",
    "H = np.stack(H, axis=0)\n",
    "\n",
    "pca = PCA(n_components=min(6, H.shape[0]-1)).fit(H)\n",
    "\n",
    "def letter_target_reduced(letter: str) -> np.ndarray:\n",
    "    vecs = [last_hidden(tpl.format(letter), NUDGE_LAYER) for tpl in LETTER_PHRASES]\n",
    "    mean_vec = np.mean(np.stack(vecs, axis=0), axis=0)\n",
    "    return pca.transform(mean_vec.reshape(1, -1)).squeeze()\n",
    "\n",
    "LETTER_TARGETS = {L: letter_target_reduced(L) for L in LETTERS}\n",
    "GLOBAL_ANTI = np.mean(np.stack([LETTER_TARGETS[L] for L in LETTERS], axis=0), axis=0)\n",
    "\n",
    "# -----------------------------\n",
    "# Symbolic spring (latent nudge)\n",
    "# -----------------------------\n",
    "def symbolic_loop(vec: np.ndarray, tgt: np.ndarray, steps=SYMB_STEPS, dt=DT) -> np.ndarray:\n",
    "    pos = vec * 12.0  # slightly smaller radius\n",
    "    vel = np.zeros_like(pos)\n",
    "    for _ in range(steps):\n",
    "        pull = PULL_STRENGTH * (tgt - pos)\n",
    "        accel = pull - GAMMA * vel\n",
    "        vel += dt * accel\n",
    "        pos += dt * vel\n",
    "    return pos\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenization helpers (letters; single-token only)\n",
    "# -----------------------------\n",
    "def encode_letter_token(letter: str) -> int:\n",
    "    # prefer a single token variant; fall back to the best scoring variant if needed\n",
    "    ids_plain = tokenizer.encode(letter, add_special_tokens=False)\n",
    "    if len(ids_plain) == 1: return ids_plain[0]\n",
    "    ids_sp = tokenizer.encode(\" \" + letter, add_special_tokens=False)\n",
    "    return ids_sp[0] if len(ids_sp) == 1 else ids_plain[0]\n",
    "\n",
    "LETTER_TOKEN = {L: encode_letter_token(L) for L in LETTERS}\n",
    "\n",
    "# -----------------------------\n",
    "# 7-class (A..G) projection cache for CPU-fast path\n",
    "# -----------------------------\n",
    "LETTER_ORDER = LETTERS  # A..G order\n",
    "LETTER_TOKEN_IDS = np.array([LETTER_TOKEN[L] for L in LETTER_ORDER], dtype=np.int64)\n",
    "\n",
    "with torch.no_grad(), torch.inference_mode():\n",
    "    W_full = model.lm_head.weight.detach().cpu().numpy()     # [V, H]\n",
    "    b_full = (model.lm_head.bias.detach().cpu().numpy()\n",
    "              if model.lm_head.bias is not None\n",
    "              else np.zeros(W_full.shape[0], dtype=W_full.dtype))\n",
    "LM_ROWS = W_full[LETTER_TOKEN_IDS, :]  # [7, H]\n",
    "LM_BIAS = b_full[LETTER_TOKEN_IDS]     # [7]\n",
    "\n",
    "def _logsumexp_1d(x: np.ndarray) -> float:\n",
    "    m = np.max(x)\n",
    "    return m + np.log(np.sum(np.exp(x - m)))\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt cache\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class PromptCache:\n",
    "    past: tuple\n",
    "    logits: torch.Tensor\n",
    "    hidden: torch.Tensor\n",
    "\n",
    "def _init_prompt_cache(prompt_ids: torch.Tensor) -> PromptCache:\n",
    "    with torch.no_grad(), torch.inference_mode():\n",
    "        out = model(prompt_ids, output_hidden_states=True, use_cache=True)\n",
    "    return PromptCache(\n",
    "        past=out.past_key_values,\n",
    "        logits=out.logits[:, -1, :].squeeze(0),\n",
    "        hidden=out.hidden_states[NUDGE_LAYER][:, -1, :].squeeze(0),\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 7-class fast scoring\n",
    "# -----------------------------\n",
    "def score_letters_fast(hidden_last: torch.Tensor, alpha: float,\n",
    "                       cal_offsets: Dict[str, float]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Returns per-letter log-prob scores for A..G only (CPU fast-path).\n",
    "    \"\"\"\n",
    "    h = hidden_last.detach().cpu().numpy()        # [H]\n",
    "    base7 = h @ LM_ROWS.T + LM_BIAS               # [7]\n",
    "\n",
    "    if alpha <= 0.0:\n",
    "        lse = _logsumexp_1d(base7)\n",
    "        logp7 = base7 - lse\n",
    "        return {L: float(logp7[i]) - cal_offsets.get(L, 0.0)\n",
    "                for i, L in enumerate(LETTER_ORDER)}\n",
    "\n",
    "    # with nudging: compute nudged 7-way vector per target letter\n",
    "    red = pca.transform(h.reshape(1, -1)).squeeze()\n",
    "    scores = {}\n",
    "    for i, L in enumerate(LETTER_ORDER):\n",
    "        tgt = LETTER_TARGETS[L] - LAMBDA_REP * GLOBAL_ANTI\n",
    "        nudged_red = symbolic_loop(red, tgt, steps=SYMB_STEPS, dt=DT)\n",
    "        inv = pca.inverse_transform(nudged_red.reshape(1, -1)).squeeze()\n",
    "        nrm = np.linalg.norm(inv)\n",
    "        if nrm > 0:\n",
    "            inv = (inv / nrm) * 5.0\n",
    "\n",
    "        nudged7 = inv @ LM_ROWS.T + LM_BIAS      # [7]\n",
    "        mix7 = (1.0 - alpha) * base7 + alpha * nudged7\n",
    "        lse = _logsumexp_1d(mix7)\n",
    "        logp7 = mix7 - lse\n",
    "        scores[L] = float(logp7[i]) - cal_offsets.get(L, 0.0)\n",
    "\n",
    "    return scores\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-prompt calibration (small K) using fast path\n",
    "# -----------------------------\n",
    "def build_calibration_offsets(alpha: float, k: int = CAL_K) -> Dict[str, float]:\n",
    "    if not USE_CALIBRATION:\n",
    "        return {L: 0.0 for L in LETTERS}\n",
    "    sums = {L: 0.0 for L in LETTERS}\n",
    "    for _ in range(k):\n",
    "        h, w = random.choice([2,3]), random.choice([2,3])\n",
    "        grid = [[random.randint(1,9) for _ in range(w)] for _ in range(h)]\n",
    "        neutral = FEWSHOT + f\"\\nInput: {grid}\\nOutput: {grid}\\nAnswer: \"\n",
    "        pid = tokenizer(neutral, return_tensors=\"pt\").to(device)[\"input_ids\"]\n",
    "\n",
    "        with torch.no_grad(), torch.inference_mode():\n",
    "            out = model(pid, output_hidden_states=True, use_cache=True)\n",
    "        hidden_last = out.hidden_states[NUDGE_LAYER][:, -1, :].squeeze(0)\n",
    "\n",
    "        # no offsets inside the call\n",
    "        fast_scores = score_letters_fast(hidden_last, alpha, {L:0.0 for L in LETTERS})\n",
    "        for L in LETTERS:\n",
    "            sums[L] += fast_scores[L]\n",
    "\n",
    "        # mean-center this prompt’s scores to remove global shift\n",
    "        mu = np.mean([sums[L] for L in LETTERS])\n",
    "        for L in LETTERS:\n",
    "            sums[L] -= mu\n",
    "\n",
    "    return {L: sums[L] / k for L in LETTERS}\n",
    "\n",
    "# -----------------------------\n",
    "# Candidates (for logging only)\n",
    "# -----------------------------\n",
    "def consistent_ops(inp, out):\n",
    "    c = []\n",
    "    for op in OPS:\n",
    "        try:\n",
    "            if EXECUTOR[op](inp) == out:\n",
    "                c.append(op)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return c\n",
    "\n",
    "# -----------------------------\n",
    "# Synthetic tasks\n",
    "# -----------------------------\n",
    "def random_grid(h, w): return [[random.randint(1,9) for _ in range(w)] for _ in range(h)]\n",
    "\n",
    "def make_tasks(n: int, seed: int = SEED):\n",
    "    rng = random.Random(seed)\n",
    "    tasks = []\n",
    "    for _ in range(n):\n",
    "        h, w = rng.choice([2,3]), rng.choice([2,3])\n",
    "        grid = [[rng.randint(1,9) for _ in range(w)] for _ in range(h)]\n",
    "        op = rng.choice(OPS)\n",
    "        expected = EXECUTOR[op](grid)\n",
    "        tasks.append((grid, op, expected))\n",
    "    return tasks\n",
    "\n",
    "# -----------------------------\n",
    "# Pass\n",
    "# -----------------------------\n",
    "def run_pass(tasks, alpha: float, tag: str):\n",
    "    eq_total = strict_total = 0\n",
    "    unamb = amb = 0\n",
    "\n",
    "    cal = build_calibration_offsets(alpha)\n",
    "\n",
    "    if PRINT_TASKS:\n",
    "        print(f\"--- {tag} (alpha={alpha:.2f}) ---\")\n",
    "\n",
    "    for i, (grid, true_op, expected) in enumerate(tasks, start=1):\n",
    "        cands = consistent_ops(grid, expected)\n",
    "        is_unamb = (len(cands) == 1)\n",
    "        if is_unamb: unamb += 1\n",
    "        else: amb += 1\n",
    "\n",
    "        prompt = FEWSHOT + f\"\\nInput: {grid}\\nOutput: {expected}\\nAnswer: \"\n",
    "        pid = tokenizer(prompt, return_tensors=\"pt\").to(device)[\"input_ids\"]\n",
    "\n",
    "        # one forward; then 7-class fast scoring\n",
    "        cache = _init_prompt_cache(pid)\n",
    "        if CPU_FAST:\n",
    "            fast_scores = score_letters_fast(cache.hidden, alpha, cal)\n",
    "            scores = sorted(fast_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        else:\n",
    "            # unsafe path shouldn't be needed; kept for completeness\n",
    "            base_logp = torch.log_softmax(cache.logits, dim=-1)\n",
    "            tmp = []\n",
    "            for L in LETTERS:\n",
    "                s = float(base_logp[LETTER_TOKEN[L]].item())\n",
    "                s -= cal.get(L, 0.0)\n",
    "                tmp.append((L, s))\n",
    "            scores = sorted(tmp, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        predL, best = scores[0]\n",
    "        second = scores[1][1] if len(scores) > 1 else float(\"-inf\")\n",
    "        margin = best - second\n",
    "\n",
    "        pred_op = LETTER_MAP[predL]\n",
    "        eq_ok = (EXECUTOR[pred_op](grid) == expected)\n",
    "        strict_ok = (pred_op == true_op)\n",
    "        eq_total += int(eq_ok); strict_total += int(strict_ok)\n",
    "\n",
    "        if PRINT_TASKS:\n",
    "            tag_task = \"Unambiguous\" if is_unamb else \"Ambiguous\"\n",
    "            print(f\"Task {i:03d} | True={true_op} | {tag_task} | Cands={cands if cands else 'ALL?'} | \"\n",
    "                  f\"{tag}={pred_op} (letter {predL}) eq={'✓' if eq_ok else '×'} \"\n",
    "                  f\"strict={'✓' if strict_ok else '×'} Δ{margin:.3f}\")\n",
    "\n",
    "    if PRINT_TASKS:\n",
    "        N = len(tasks)\n",
    "        print(f\"\\\\n=== {tag} Summary ===\")\n",
    "        print(f\"Prefilter used   : {False}\")\n",
    "        print(f\"Unambiguous pairs: {unamb}\")\n",
    "        print(f\"Ambiguous pairs  : {amb}\")\n",
    "        print(f\"Equivalence Acc  : {eq_total}/{N} = {100*eq_total/N:.1f}%\")\n",
    "        print(f\"Strict-label Acc : {strict_total}/{N} = {100*strict_total/N:.1f}%\\\\n\")\n",
    "\n",
    "    return {\"N\\\": len(tasks), \\\"eq\\\": eq_total, \\\"strict\\\": strict_total\"}\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    tasks = make_tasks(N_TASKS, seed=SEED)\n",
    "    stock  = run_pass(tasks, alpha=0.0, tag=\\\"Stock\\\")\n",
    "    warped = run_pass(tasks, alpha=ALPHA_WARPED, tag=\\\"Warped\\\")\n",
    "\n",
    "    print(\"=== Side-by-side Summary (same tasks) ===\")\n",
    "    print(f\"N={stock['N']} | Prefilter={False} | Calibration={USE_CALIBRATION}\"\n",
    "          f\"| Alpha_warped={ALPHA_WARPED} | NUDGE_LAYER={NUDGE_LAYER} | CAL_K={CAL_K}\")\n",
    "    print(f\"Equivalence Acc  : Stock {stock['eq']}/{stock['N']} = {100*stock['eq']/stock['N']:.1f}% | \"\n",
    "          f\"Warped {warped['eq']}/{warped['N']} = {100*warped['eq']/warped['N']:.1f}%\")\n",
    "    print(f\"Strict-label Acc : Stock {stock['strict']}/{stock['N']} = {100*stock['strict']/stock['N']:.1f}% | \"\n",
    "          f\"Warped {warped['strict']}/{warped['N']} = {100*warped['strict']/warped['N']:.1f}%\")\n",
    "\n",
    "if __name__ == \\\"__main__\\\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9161d4-b92c-4232-9b90-bd4ad37b64e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
