{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c569c0e-cb1d-4f4c-aa31-eebc8d6b0b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PCA] samples=3 feat=768 cap=3 -> n=2 (cum var=1.0000, whiten=True)\n",
      "Reduced dims: 2\n",
      "Final full-latent norm: 178.7329470185916\n",
      "PCA n_components: 2\n",
      "Explained variance (cumulative): 1.0\n",
      "[ARC] Endpoint nearest anchor: 0 (rotate) at distance 6.6196\n",
      "[ARC] Distances to anchors: [6.6196 8.4764 7.0856]\n",
      "[ARC] Predicted output grid: [[3, 1], [4, 2]]\n"
     ]
    }
   ],
   "source": [
    "# step9_hybrid_geodesic.py\n",
    "# CPU-only, NumPy + transformers. Precision Step 9 (geodesics), optional tiny damping.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# ---------- Config ----------\n",
    "REDUCED_VAR=0.99\n",
    "LAMBDA = 0.35     # was 0.2\n",
    "DT = 0.03         # was 0.04\n",
    "STEPS = 600       # was 400\n",
    "GAMMA = 0.02      # keep small damping\n",
    "GAMMA = 0.02         # tiny damping for numerical stability (0 -> pure geodesic)\n",
    "SEED = 42\n",
    "EPS = 1e-6             # used in potential/grad\n",
    "\n",
    "MODE = \"geodesic\"\n",
    "\n",
    "# --- PCA ---\n",
    "REDUCED_VAR = 0.99\n",
    "MIN_DIMS = 8\n",
    "MAX_DIMS = 16\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ---------- Latent extraction ----------\n",
    "_device = torch.device(\"cpu\")\n",
    "\n",
    "_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(_device)\n",
    "_model.eval()\n",
    "\n",
    "def get_latent(prompt: str) -> np.ndarray:\n",
    "    \"\"\"Mean-pool last hidden state as latent.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        toks = _tokenizer(prompt, return_tensors=\"pt\").to(_device)\n",
    "        out = _model(**toks, output_hidden_states=True)\n",
    "        hs = out.hidden_states[-1][0].cpu().numpy()  # (seq, hidden)\n",
    "    return hs.mean(axis=0)  # (hidden,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pca_fit_transform(vectors, target_var=0.95, MIN_DIMS=2, MAX_DIMS=8, verbose=True):\n",
    "    \"\"\"\n",
    "    vectors: (n_samples, n_features)\n",
    "    Chooses n_components safely based on available samples/features and target variance.\n",
    "    \"\"\"\n",
    "    vectors = np.asarray(vectors)\n",
    "    n_samples, n_features = vectors.shape\n",
    "    n_cap = min(n_samples, n_features)          # hard cap from sklearn\n",
    "    if n_cap <= 0:\n",
    "        raise ValueError(\"Empty training set for PCA.\")\n",
    "    \n",
    "    # First fit with full dimensionality up to the cap to measure variance captured.\n",
    "    probe_n = min(n_cap, MAX_DIMS)              # don't waste time probing huge dims\n",
    "    pca_probe = PCA(n_components=probe_n, whiten=False, svd_solver=\"full\").fit(vectors)\n",
    "    cumvar = np.cumsum(pca_probe.explained_variance_ratio_)\n",
    "    # Smallest k meeting target variance (or probe_n if target not reached)\n",
    "    k = int(np.searchsorted(cumvar, target_var) + 1)\n",
    "    \n",
    "    # Final n: respect MIN/MAX and the cap\n",
    "    n = max(MIN_DIMS, min(MAX_DIMS, k))\n",
    "    n = min(n, n_cap)\n",
    "    \n",
    "    # If we only have 1–2 samples/features, we may be forced to 1D.\n",
    "    if n < MIN_DIMS and n_cap >= 1:\n",
    "        # fall back gracefully; keep whiten=False when n is tiny\n",
    "        n = n_cap\n",
    "    \n",
    "    whiten = (n >= 2)   # avoid whitening in degenerate 1D cases\n",
    "    pca = PCA(n_components=n, whiten=whiten, svd_solver=\"full\").fit(vectors)\n",
    "    Z = pca.transform(vectors)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[PCA] samples={n_samples} feat={n_features} cap={n_cap} \"\n",
    "              f\"-> n={pca.n_components_} (cum var={cumvar[min(n-1, len(cumvar)-1)]:.4f}, \"\n",
    "              f\"whiten={whiten})\")\n",
    "        if pca.n_components_ < 2:\n",
    "            print(\"NOTE: PCA ended up 1D due to limited samples/features. \"\n",
    "                  \"Add more training prompts or lower MIN_DIMS if downstream expects ≥2D.\")\n",
    "    return pca, Z\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Conformally flat metric from multi-mass potential ----------\n",
    "def potential(x: np.ndarray, centers: np.ndarray, masses: np.ndarray) -> float:\n",
    "    # V(x) = - sum_i M_i / (||x - c_i|| + eps)\n",
    "    diffs = x[None, :] - centers\n",
    "    dists = np.linalg.norm(diffs, axis=1) + EPS\n",
    "    return -np.sum(masses / dists)\n",
    "\n",
    "def grad_potential(x: np.ndarray, centers: np.ndarray, masses: np.ndarray) -> np.ndarray:\n",
    "    # ∇V = - sum_i M_i * (-(x - c_i)) / (||x - c_i||^3 + eps)\n",
    "    diffs = x[None, :] - centers  # (k,d)\n",
    "    dists = np.linalg.norm(diffs, axis=1) + EPS  # (k,)\n",
    "    # d/dx (1/r) = - (x-c)/r^3\n",
    "    terms = masses[:, None] * diffs / (dists**3)[:, None]  # (k,d)\n",
    "    return -np.sum(terms, axis=0)\n",
    "\n",
    "def lnphi_and_grad(x: np.ndarray, centers: np.ndarray, masses: np.ndarray):\n",
    "    V = potential(x, centers, masses)\n",
    "    lnphi = LAMBDA * V\n",
    "    # ∇lnφ = λ ∇V\n",
    "    g_lnphi = LAMBDA * grad_potential(x, centers, masses)\n",
    "    return lnphi, g_lnphi\n",
    "\n",
    "def christoffel_conformal(x: np.ndarray, v: np.ndarray, centers: np.ndarray, masses: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Γ^i_{jk} v^j v^k for g_ij = φ^2 δ_ij with φ = exp(λV).\n",
    "    Γ^i_{jk} = δ^i_j ∂_k lnφ + δ^i_k ∂_j lnφ - δ_jk ∂^i lnφ\n",
    "    Contract with v^j v^k: 2 v_i (v·∇lnφ) - ||v||^2 ∂^i lnφ\n",
    "    Returned as vector with upper index i.\n",
    "    \"\"\"\n",
    "    _, grad_lnphi = lnphi_and_grad(x, centers, masses)\n",
    "    v_dot = float(np.dot(v, grad_lnphi))\n",
    "    v_norm2 = float(np.dot(v, v))\n",
    "    return 2.0 * v * v_dot - v_norm2 * grad_lnphi\n",
    "\n",
    "def rk4_step(x: np.ndarray, v: np.ndarray, dt: float, centers: np.ndarray, masses: np.ndarray):\n",
    "    \"\"\"\n",
    "    Geodesic ODE in coordinates (x, v):\n",
    "      dx/dτ = v\n",
    "      dv/dτ = - Γ(x)[v,v] - γ v\n",
    "    \"\"\"\n",
    "    def a(x_, v_):\n",
    "        return -christoffel_conformal(x_, v_, centers, masses) - GAMMA * v_\n",
    "\n",
    "    k1x = v\n",
    "    k1v = a(x, v)\n",
    "\n",
    "    k2x = v + 0.5 * dt * k1v\n",
    "    k2v = a(x + 0.5 * dt * k1x, v + 0.5 * dt * k1v)\n",
    "\n",
    "    k3x = v + 0.5 * dt * k2v\n",
    "    k3v = a(x + 0.5 * dt * k2x, v + 0.5 * dt * k2v)\n",
    "\n",
    "    k4x = v + dt * k3v\n",
    "    k4v = a(x + dt * k3x, v + dt * k3v)\n",
    "\n",
    "    x_new = x + (dt / 6.0) * (k1x + 2*k2x + 2*k3x + k4x)\n",
    "    v_new = v + (dt / 6.0) * (k1v + 2*k2v + 2*k3v + k4v)\n",
    "    return x_new, v_new\n",
    "\n",
    "def integrate_geodesic(x0: np.ndarray, v0: np.ndarray, centers: np.ndarray, masses: np.ndarray,\n",
    "                       steps: int = STEPS, dt: float = DT):\n",
    "    x, v = x0.copy(), v0.copy()\n",
    "    traj = [x.copy()]\n",
    "    for _ in range(steps):\n",
    "        x, v = rk4_step(x, v, dt, centers, masses)\n",
    "        traj.append(x.copy())\n",
    "    return np.array(traj)\n",
    "\n",
    "# # ---------- Anchors / centers ----------\n",
    "# def compute_anchors(train_latents_reduced: np.ndarray, labels: list[str]):\n",
    "#     \"\"\"\n",
    "#     Group training examples by transformation label and return centers + masses.\n",
    "#     If you don’t have labels, use k-means or PCA cluster means as centers.\n",
    "#     \"\"\"\n",
    "#     kinds = sorted(set(labels))\n",
    "#     centers = []\n",
    "#     masses = []\n",
    "#     for k in kinds:\n",
    "#         group = train_latents_reduced[[i for i,l in enumerate(labels) if l==k]]\n",
    "#         c = group.mean(axis=0)\n",
    "#         centers.append(c)\n",
    "#         # simple heuristic mass by group compactness (inverse spread)\n",
    "#         spread = float(np.mean(np.linalg.norm(group - c, axis=1)) + 1e-6)\n",
    "#         masses.append(1.0 / spread)\n",
    "#     centers = np.vstack(centers)\n",
    "#     masses = np.array(masses, dtype=float)\n",
    "#     # normalize masses\n",
    "#     masses = masses / (np.sum(masses) + 1e-9)\n",
    "#     return centers, masses\n",
    "\n",
    "# --- Compute anchor centers and semantic masses ---\n",
    "def compute_anchors(Z, labels):\n",
    "    uniq = sorted(set(labels))\n",
    "    centers, masses = [], []\n",
    "    for u in uniq:\n",
    "        idxs = [i for i,l in enumerate(labels) if l == u]\n",
    "        group = Z[idxs]\n",
    "        c = group.mean(axis=0)\n",
    "        centers.append(c)\n",
    "        spread = float(np.mean(np.linalg.norm(group - c, axis=1)) + 1e-6)\n",
    "        masses.append(1.0 / spread)  # tighter -> heavier mass\n",
    "    centers = np.stack(centers, axis=0)\n",
    "    masses = np.array(masses, dtype=float)\n",
    "    masses /= (masses.sum() + 1e-9)\n",
    "    return centers, masses\n",
    "\n",
    "\n",
    "def classify_endpoint(z_final_red, centers):\n",
    "    d = np.linalg.norm(centers - z_final_red, axis=1)\n",
    "    j = int(np.argmin(d))\n",
    "    return j, float(d[j])\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Example end-to-end ----------\n",
    "def solve_with_geodesics(prompt: str,\n",
    "                         pca: PCA,\n",
    "                         anchor_centers_red: np.ndarray,\n",
    "                         masses: np.ndarray):\n",
    "    z_full = get_latent(prompt)                 # (hidden,)\n",
    "    z_red  = pca.transform(z_full[None, :])[0]  # (d,)\n",
    "\n",
    "    # Start slightly displaced with small initial velocity toward negative grad V\n",
    "    lnphi, g_lnphi = lnphi_and_grad(z_red, anchor_centers_red, masses)\n",
    "    v0 = -0.1 * g_lnphi / (np.linalg.norm(g_lnphi) + 1e-9)\n",
    "\n",
    "    traj = integrate_geodesic(z_red, v0, anchor_centers_red, masses, steps=STEPS, dt=DT)\n",
    "    z_final_red = traj[-1]\n",
    "    # Map back to full space (optional): x_full ≈ z_red * P^T + mean\n",
    "    z_full_final = pca.inverse_transform(z_final_red[None, :])[0]\n",
    "    return z_full_final, traj\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# ARC warp-interference solver\n",
    "# ----------------------------\n",
    "\n",
    "# 1) Minimal ARC transforms we’ll support right now\n",
    "def rot90_cw(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.rot90(g, k=3).tolist()\n",
    "\n",
    "def rot180(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.rot90(g, k=2).tolist()\n",
    "\n",
    "def rot270_cw(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.rot90(g, k=1).tolist()\n",
    "\n",
    "def flip_h(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.flip(g, axis=1).tolist()\n",
    "\n",
    "def flip_v(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.flip(g, axis=0).tolist()\n",
    "\n",
    "# Map anchor index -> transform; adjust to your labels/anchor order\n",
    "TRANSFORM_BY_LABEL = {\n",
    "    \"rotate\": rot90_cw,       # label 0\n",
    "    \"flip_h\": flip_h,         # label 1\n",
    "    \"flip_v\": flip_v,         # label 2\n",
    "    # add more labels/anchors as you train them:\n",
    "    # \"rot180\": rot180,\n",
    "    # \"rot270\": rot270_cw,\n",
    "}\n",
    "\n",
    "# If you created anchors via sorted(set(labels)), keep the same order here:\n",
    "ANCHOR_LABELS_IN_ORDER = [\"rotate\", \"flip_h\", \"flip_v\"]  # <- keep in sync with your training labels\n",
    "\n",
    "def grid_to_prompt(grid):\n",
    "    # simple text promptization so GPT-2 latent “sees” the exact instance\n",
    "    return f\"Identify the pattern: Input grid {grid} -> Output ? (choose rotate 90° cw, flip_h, or flip_v).\"\n",
    "\n",
    "def classify_endpoint(z_final_red, centers):\n",
    "    d = np.linalg.norm(centers - z_final_red, axis=1)\n",
    "    j = int(np.argmin(d))\n",
    "    return j, float(d[j]), d\n",
    "\n",
    "def arc_latent_for_grid(grid, pca):\n",
    "    prompt = grid_to_prompt(grid)\n",
    "    z_full = get_latent(prompt)\n",
    "    z_red = pca.transform(z_full.reshape(1, -1))[0]\n",
    "    return z_red\n",
    "\n",
    "def run_warp_interference(z_red, centers, masses, steps, dt, mode=\"geodesic\"):\n",
    "    # build initial velocity along −∇lnφ (fallback to small random)\n",
    "    _, g_lnphi = lnphi_and_grad(z_red, centers, masses)\n",
    "    g_norm = float(np.linalg.norm(g_lnphi))\n",
    "    if g_norm < 1e-8:\n",
    "        rng = np.random.default_rng(0)\n",
    "        v0 = rng.normal(size=z_red.shape).astype(float)\n",
    "        v0 /= (np.linalg.norm(v0) + 1e-9)\n",
    "        v0 *= 0.05\n",
    "    else:\n",
    "        v0 = -g_lnphi / (g_norm + 1e-9) * 0.1\n",
    "\n",
    "    if mode == \"geodesic\":\n",
    "        traj = integrate_geodesic(x0=z_red, v0=v0, centers=centers, masses=masses, steps=steps, dt=dt)\n",
    "        z_final = traj[-1]\n",
    "    else:\n",
    "        # Stage-9 linearized nudge (if you kept integrate_nudge from earlier)\n",
    "        traj = integrate_nudge(z_red, centers, masses, steps=350, dt=0.05, k=2.0, gamma=0.2)\n",
    "        z_final = traj[-1]\n",
    "\n",
    "    return z_final\n",
    "\n",
    "def solve_arc_task(input_grid, *, verbose=True):\n",
    "    \"\"\"\n",
    "    input_grid: e.g. [[1,2],[3,4]] or a 3x3 integer grid.\n",
    "    Returns: predicted_output_grid (same shape as input)\n",
    "    \"\"\"\n",
    "    # 1) embed this specific instance\n",
    "    z_red = arc_latent_for_grid(input_grid, pca)\n",
    "\n",
    "    # 2) warp-interference geodesic to pick the anchor\n",
    "    z_final = run_warp_interference(z_red, anchor_centers_red, masses, steps=STEPS, dt=DT, mode=MODE)\n",
    "\n",
    "    # 3) nearest anchor = chosen transform\n",
    "    j, dmin, all_d = classify_endpoint(z_final, anchor_centers_red)\n",
    "    chosen_label = ANCHOR_LABELS_IN_ORDER[j]\n",
    "    transform = TRANSFORM_BY_LABEL.get(chosen_label, None)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[ARC] Endpoint nearest anchor: {j} ({chosen_label}) at distance {dmin:.4f}\")\n",
    "        print(f\"[ARC] Distances to anchors:\", np.array_str(all_d, precision=4, suppress_small=True))\n",
    "\n",
    "    if transform is None:\n",
    "        raise ValueError(f\"No transform mapped for anchor label '{chosen_label}'\")\n",
    "\n",
    "    # 4) apply transform to the grid\n",
    "    output_grid = transform(input_grid)\n",
    "    return output_grid\n",
    "\n",
    "def anchor_confidence(distances, tau=2.0):\n",
    "    # lower distance → higher confidence via softmax(-d/tau)\n",
    "    distances = np.asarray(distances, dtype=float)\n",
    "    s = np.exp(-(distances - distances.min()) / max(1e-9, tau))\n",
    "    p = s / (s.sum() + 1e-9)\n",
    "    j = int(np.argmin(distances))\n",
    "    return j, float(p[j]), p\n",
    "\n",
    "\n",
    "# Build 12 toy cases covering rotate / flip_h / flip_v evenly\n",
    "def make_cases():\n",
    "    # small 2x2 and 3x3 to vary structure\n",
    "    cases = [\n",
    "        # rotate 90° cw\n",
    "        {\"grid\": [[1,2],[3,4]], \"label\":\"rotate\"},\n",
    "        {\"grid\": [[5,6],[7,8]], \"label\":\"rotate\"},\n",
    "        {\"grid\": [[1,0,2],[0,1,0],[2,0,1]], \"label\":\"rotate\"},\n",
    "        {\"grid\": [[9,8,7],[6,5,4],[3,2,1]], \"label\":\"rotate\"},\n",
    "        # flip_h\n",
    "        {\"grid\": [[1,2],[3,4]], \"label\":\"flip_h\"},\n",
    "        {\"grid\": [[5,6],[7,8]], \"label\":\"flip_h\"},\n",
    "        {\"grid\": [[1,0,2],[0,1,0],[2,0,1]], \"label\":\"flip_h\"},\n",
    "        {\"grid\": [[9,8,7],[6,5,4],[3,2,1]], \"label\":\"flip_h\"},\n",
    "        # flip_v\n",
    "        {\"grid\": [[1,2],[3,4]], \"label\":\"flip_v\"},\n",
    "        {\"grid\": [[5,6],[7,8]], \"label\":\"flip_v\"},\n",
    "        {\"grid\": [[1,0,2],[0,1,0],[2,0,1]], \"label\":\"flip_v\"},\n",
    "        {\"grid\": [[9,8,7],[6,5,4],[3,2,1]], \"label\":\"flip_v\"},\n",
    "    ]\n",
    "    # attach ground-truth transformed grids using your transform map\n",
    "    out = []\n",
    "    for c in cases:\n",
    "        tfunc = TRANSFORM_BY_LABEL[c[\"label\"]]\n",
    "        out.append({\"grid\": c[\"grid\"], \"label\": c[\"label\"], \"target\": tfunc(c[\"grid\"])})\n",
    "    return out\n",
    "\n",
    "def run_arc_benchmark_12(verbose=True, tau=2.0):\n",
    "    cases = make_cases()\n",
    "    correct = 0\n",
    "    confs = []\n",
    "    per_case = []\n",
    "\n",
    "    for idx, case in enumerate(cases, 1):\n",
    "        z_red = arc_latent_for_grid(case[\"grid\"], pca)\n",
    "        z_final = run_warp_interference(z_red, anchor_centers_red, masses, steps=STEPS, dt=DT, mode=MODE)\n",
    "        j, dmin, dists = classify_endpoint(z_final, anchor_centers_red)\n",
    "        j_hat, conf, probs = anchor_confidence(dists, tau=tau)\n",
    "        pred_label = ANCHOR_LABELS_IN_ORDER[j_hat]\n",
    "        pred_grid = TRANSFORM_BY_LABEL[pred_label](case[\"grid\"])\n",
    "\n",
    "        ok = (pred_label == case[\"label\"]) and (pred_grid == case[\"target\"])\n",
    "        correct += int(ok)\n",
    "        confs.append(conf)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[{idx:02d}] true={case['label']:7s} pred={pred_label:7s} \"\n",
    "                  f\"ok={ok} dist*={dmin:.3f} conf={conf:.3f} probs={np.array_str(probs, precision=3)}\")\n",
    "\n",
    "        per_case.append({\n",
    "            \"idx\": idx,\n",
    "            \"true\": case[\"label\"],\n",
    "            \"pred\": pred_label,\n",
    "            \"ok\": ok,\n",
    "            \"conf\": conf,\n",
    "            \"dmin\": dmin,\n",
    "        })\n",
    "\n",
    "    acc = correct / len(cases)\n",
    "    mean_conf = float(np.mean(confs)) if confs else 0.0\n",
    "    print(f\"\\n[ARC-12] Accuracy: {acc*100:.1f}% | Mean confidence: {mean_conf:.3f} | Mode={MODE}\")\n",
    "    return {\"accuracy\": acc, \"mean_confidence\": mean_conf, \"details\": per_case}\n",
    "\n",
    "\n",
    "# ------------- Quick smoke test -------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- minimal demo wiring (replace with your train set) ---\n",
    "    train_prompts = [\n",
    "        \"Identify the pattern: rotate 90° clockwise ...\",\n",
    "        \"Identify the pattern: flip horizontally ...\",\n",
    "        \"Identify the pattern: flip vertically ...\",\n",
    "        # add more task archetypes...\n",
    "    ]\n",
    "    labels = [\"rotate\", \"flip_h\", \"flip_v\"]\n",
    "\n",
    "    train_latents = np.stack([get_latent(p) for p in train_prompts], axis=0)\n",
    "    # when calling pca_fit_transform for tiny sets\n",
    "    pca, Z = pca_fit_transform(train_latents, target_var=0.99, MIN_DIMS=2, MAX_DIMS=3, verbose=True)\n",
    "\n",
    "    if pca.n_components_ < 2:\n",
    "        print(\"NOTE: PCA ended up 1D. Add more training prompts or lower-level pooling diversity.\")\n",
    "\n",
    "\n",
    "\n",
    "    centers_red, masses = compute_anchors(Z, labels)\n",
    "\n",
    "    masses = masses / (masses.sum() + 1e-9)\n",
    "    masses *= 3.0     # try 2–5; adjust if too aggressive\n",
    "\n",
    "\n",
    "    test_prompt = \"Identify the pattern: Input grid [[1,2],[3,4]] -> ...\"\n",
    "    full_final, traj = solve_with_geodesics(test_prompt, pca, centers_red, masses)\n",
    "    print(\"Reduced dims:\", pca.n_components_)\n",
    "    print(\"Final full-latent norm:\", np.linalg.norm(full_final))\n",
    "\n",
    "    print(\"PCA n_components:\", pca.n_components_)\n",
    "    print(\"Explained variance (cumulative):\", np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "    # Choose a starting prompt to evaluate/integrate from\n",
    "    # (1) if you have a specific eval prompt:\n",
    "    eval_prompt = \"Rotate [[2,3],[4,5]] by 90°\"  # <- replace as needed\n",
    "    latent_full = get_latent(eval_prompt)        # shape: (D,)\n",
    "    z_red = pca.transform(latent_full.reshape(1, -1))[0]  # shape: (pca.n_components_,)\n",
    "\n",
    "    # build reduced anchors\n",
    "    anchor_centers_red, masses = compute_anchors(Z, labels)\n",
    "    \n",
    "    demo_grid = [[1,2],[3,4]]  # classic toy; true label often \"rotate\" for some tasks\n",
    "    pred = solve_arc_task(demo_grid, verbose=True)\n",
    "    print(\"[ARC] Predicted output grid:\", pred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ac528-9cb1-4d36-9a07-1ddf6016ad22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
