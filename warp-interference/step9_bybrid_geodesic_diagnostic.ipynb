{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b1415b-29c3-4a4d-99a8-57264360cf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PCA] samples=3 feat=768 cap=3 -> n=2 (cum var=1.0000, whiten=True)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EPS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 373\u001b[0m\n\u001b[1;32m    369\u001b[0m masses \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3.0\u001b[39m     \u001b[38;5;66;03m# try 2–5; adjust if too aggressive\u001b[39;00m\n\u001b[1;32m    372\u001b[0m test_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdentify the pattern: Input grid [[1,2],[3,4]] -> ...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 373\u001b[0m full_final, traj \u001b[38;5;241m=\u001b[39m \u001b[43msolve_with_geodesics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters_red\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReduced dims:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pca\u001b[38;5;241m.\u001b[39mn_components_)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal full-latent norm:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(full_final))\n",
      "Cell \u001b[0;32mIn[1], line 215\u001b[0m, in \u001b[0;36msolve_with_geodesics\u001b[0;34m(prompt, pca, anchor_centers_red, masses)\u001b[0m\n\u001b[1;32m    212\u001b[0m z_red  \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(z_full[\u001b[38;5;28;01mNone\u001b[39;00m, :])[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (d,)\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Start slightly displaced with small initial velocity toward negative grad V\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m lnphi, g_lnphi \u001b[38;5;241m=\u001b[39m \u001b[43mlnphi_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_red\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchor_centers_red\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m v0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m g_lnphi \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(g_lnphi) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-9\u001b[39m)\n\u001b[1;32m    218\u001b[0m traj \u001b[38;5;241m=\u001b[39m integrate_geodesic(z_red, v0, anchor_centers_red, masses, steps\u001b[38;5;241m=\u001b[39mSTEPS, dt\u001b[38;5;241m=\u001b[39mDT)\n",
      "Cell \u001b[0;32mIn[1], line 108\u001b[0m, in \u001b[0;36mlnphi_and_grad\u001b[0;34m(x, centers, masses)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlnphi_and_grad\u001b[39m(x: np\u001b[38;5;241m.\u001b[39mndarray, centers: np\u001b[38;5;241m.\u001b[39mndarray, masses: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 108\u001b[0m     V \u001b[38;5;241m=\u001b[39m \u001b[43mpotential\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     lnphi \u001b[38;5;241m=\u001b[39m LAMBDA \u001b[38;5;241m*\u001b[39m V\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# ∇lnφ = λ ∇V\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 96\u001b[0m, in \u001b[0;36mpotential\u001b[0;34m(x, centers, masses)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpotential\u001b[39m(x: np\u001b[38;5;241m.\u001b[39mndarray, centers: np\u001b[38;5;241m.\u001b[39mndarray, masses: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# V(x) = - sum_i M_i / (||x - c_i|| + eps)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     diffs \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;28;01mNone\u001b[39;00m, :] \u001b[38;5;241m-\u001b[39m centers\n\u001b[0;32m---> 96\u001b[0m     dists \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(diffs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[43mEPS\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(masses \u001b[38;5;241m/\u001b[39m dists)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EPS' is not defined"
     ]
    }
   ],
   "source": [
    "# step9_hybrid_geodesic.py\n",
    "# CPU-only, NumPy + transformers. Precision Step 9 (geodesics), optional tiny damping.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# ---------- Config ----------\n",
    "REDUCED_VAR=0.99\n",
    "LAMBDA = 0.35     # was 0.2\n",
    "DT = 0.03         # was 0.04\n",
    "STEPS = 600       # was 400\n",
    "GAMMA = 0.02      # keep small damping\n",
    "GAMMA = 0.02         # tiny damping for numerical stability (0 -> pure geodesic)\n",
    "SEED = 42\n",
    "\n",
    "MODE = \"geodesic\"\n",
    "\n",
    "# --- PCA ---\n",
    "REDUCED_VAR = 0.99\n",
    "MIN_DIMS = 8\n",
    "MAX_DIMS = 16\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ---------- Latent extraction ----------\n",
    "_device = torch.device(\"cpu\")\n",
    "\n",
    "_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(_device)\n",
    "_model.eval()\n",
    "\n",
    "def get_latent(prompt: str) -> np.ndarray:\n",
    "    \"\"\"Mean-pool last hidden state as latent.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        toks = _tokenizer(prompt, return_tensors=\"pt\").to(_device)\n",
    "        out = _model(**toks, output_hidden_states=True)\n",
    "        hs = out.hidden_states[-1][0].cpu().numpy()  # (seq, hidden)\n",
    "    return hs.mean(axis=0)  # (hidden,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pca_fit_transform(vectors, target_var=0.95, MIN_DIMS=2, MAX_DIMS=8, verbose=True):\n",
    "    \"\"\"\n",
    "    vectors: (n_samples, n_features)\n",
    "    Chooses n_components safely based on available samples/features and target variance.\n",
    "    \"\"\"\n",
    "    vectors = np.asarray(vectors)\n",
    "    n_samples, n_features = vectors.shape\n",
    "    n_cap = min(n_samples, n_features)          # hard cap from sklearn\n",
    "    if n_cap <= 0:\n",
    "        raise ValueError(\"Empty training set for PCA.\")\n",
    "    \n",
    "    # First fit with full dimensionality up to the cap to measure variance captured.\n",
    "    probe_n = min(n_cap, MAX_DIMS)              # don't waste time probing huge dims\n",
    "    pca_probe = PCA(n_components=probe_n, whiten=False, svd_solver=\"full\").fit(vectors)\n",
    "    cumvar = np.cumsum(pca_probe.explained_variance_ratio_)\n",
    "    # Smallest k meeting target variance (or probe_n if target not reached)\n",
    "    k = int(np.searchsorted(cumvar, target_var) + 1)\n",
    "    \n",
    "    # Final n: respect MIN/MAX and the cap\n",
    "    n = max(MIN_DIMS, min(MAX_DIMS, k))\n",
    "    n = min(n, n_cap)\n",
    "    \n",
    "    # If we only have 1–2 samples/features, we may be forced to 1D.\n",
    "    if n < MIN_DIMS and n_cap >= 1:\n",
    "        # fall back gracefully; keep whiten=False when n is tiny\n",
    "        n = n_cap\n",
    "    \n",
    "    whiten = (n >= 2)   # avoid whitening in degenerate 1D cases\n",
    "    pca = PCA(n_components=n, whiten=whiten, svd_solver=\"full\").fit(vectors)\n",
    "    Z = pca.transform(vectors)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[PCA] samples={n_samples} feat={n_features} cap={n_cap} \"\n",
    "              f\"-> n={pca.n_components_} (cum var={cumvar[min(n-1, len(cumvar)-1)]:.4f}, \"\n",
    "              f\"whiten={whiten})\")\n",
    "        if pca.n_components_ < 2:\n",
    "            print(\"NOTE: PCA ended up 1D due to limited samples/features. \"\n",
    "                  \"Add more training prompts or lower MIN_DIMS if downstream expects ≥2D.\")\n",
    "    return pca, Z\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Conformally flat metric from multi-mass potential ----------\n",
    "def potential(x: np.ndarray, centers: np.ndarray, masses: np.ndarray) -> float:\n",
    "    # V(x) = - sum_i M_i / (||x - c_i|| + eps)\n",
    "    diffs = x[None, :] - centers\n",
    "    dists = np.linalg.norm(diffs, axis=1) + EPS\n",
    "    return -np.sum(masses / dists)\n",
    "\n",
    "def grad_potential(x: np.ndarray, centers: np.ndarray, masses: np.ndarray) -> np.ndarray:\n",
    "    # ∇V = - sum_i M_i * (-(x - c_i)) / (||x - c_i||^3 + eps)\n",
    "    diffs = x[None, :] - centers  # (k,d)\n",
    "    dists = np.linalg.norm(diffs, axis=1) + EPS  # (k,)\n",
    "    # d/dx (1/r) = - (x-c)/r^3\n",
    "    terms = masses[:, None] * diffs / (dists**3)[:, None]  # (k,d)\n",
    "    return -np.sum(terms, axis=0)\n",
    "\n",
    "def lnphi_and_grad(x: np.ndarray, centers: np.ndarray, masses: np.ndarray):\n",
    "    V = potential(x, centers, masses)\n",
    "    lnphi = LAMBDA * V\n",
    "    # ∇lnφ = λ ∇V\n",
    "    g_lnphi = LAMBDA * grad_potential(x, centers, masses)\n",
    "    return lnphi, g_lnphi\n",
    "\n",
    "def christoffel_conformal(x: np.ndarray, v: np.ndarray, centers: np.ndarray, masses: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Γ^i_{jk} v^j v^k for g_ij = φ^2 δ_ij with φ = exp(λV).\n",
    "    Γ^i_{jk} = δ^i_j ∂_k lnφ + δ^i_k ∂_j lnφ - δ_jk ∂^i lnφ\n",
    "    Contract with v^j v^k: 2 v_i (v·∇lnφ) - ||v||^2 ∂^i lnφ\n",
    "    Returned as vector with upper index i.\n",
    "    \"\"\"\n",
    "    _, grad_lnphi = lnphi_and_grad(x, centers, masses)\n",
    "    v_dot = float(np.dot(v, grad_lnphi))\n",
    "    v_norm2 = float(np.dot(v, v))\n",
    "    return 2.0 * v * v_dot - v_norm2 * grad_lnphi\n",
    "\n",
    "def rk4_step(x: np.ndarray, v: np.ndarray, dt: float, centers: np.ndarray, masses: np.ndarray):\n",
    "    \"\"\"\n",
    "    Geodesic ODE in coordinates (x, v):\n",
    "      dx/dτ = v\n",
    "      dv/dτ = - Γ(x)[v,v] - γ v\n",
    "    \"\"\"\n",
    "    def a(x_, v_):\n",
    "        return -christoffel_conformal(x_, v_, centers, masses) - GAMMA * v_\n",
    "\n",
    "    k1x = v\n",
    "    k1v = a(x, v)\n",
    "\n",
    "    k2x = v + 0.5 * dt * k1v\n",
    "    k2v = a(x + 0.5 * dt * k1x, v + 0.5 * dt * k1v)\n",
    "\n",
    "    k3x = v + 0.5 * dt * k2v\n",
    "    k3v = a(x + 0.5 * dt * k2x, v + 0.5 * dt * k2v)\n",
    "\n",
    "    k4x = v + dt * k3v\n",
    "    k4v = a(x + dt * k3x, v + dt * k3v)\n",
    "\n",
    "    x_new = x + (dt / 6.0) * (k1x + 2*k2x + 2*k3x + k4x)\n",
    "    v_new = v + (dt / 6.0) * (k1v + 2*k2v + 2*k3v + k4v)\n",
    "    return x_new, v_new\n",
    "\n",
    "def integrate_geodesic(x0: np.ndarray, v0: np.ndarray, centers: np.ndarray, masses: np.ndarray,\n",
    "                       steps: int = STEPS, dt: float = DT):\n",
    "    x, v = x0.copy(), v0.copy()\n",
    "    traj = [x.copy()]\n",
    "    for _ in range(steps):\n",
    "        x, v = rk4_step(x, v, dt, centers, masses)\n",
    "        traj.append(x.copy())\n",
    "    return np.array(traj)\n",
    "\n",
    "# # ---------- Anchors / centers ----------\n",
    "# def compute_anchors(train_latents_reduced: np.ndarray, labels: list[str]):\n",
    "#     \"\"\"\n",
    "#     Group training examples by transformation label and return centers + masses.\n",
    "#     If you don’t have labels, use k-means or PCA cluster means as centers.\n",
    "#     \"\"\"\n",
    "#     kinds = sorted(set(labels))\n",
    "#     centers = []\n",
    "#     masses = []\n",
    "#     for k in kinds:\n",
    "#         group = train_latents_reduced[[i for i,l in enumerate(labels) if l==k]]\n",
    "#         c = group.mean(axis=0)\n",
    "#         centers.append(c)\n",
    "#         # simple heuristic mass by group compactness (inverse spread)\n",
    "#         spread = float(np.mean(np.linalg.norm(group - c, axis=1)) + 1e-6)\n",
    "#         masses.append(1.0 / spread)\n",
    "#     centers = np.vstack(centers)\n",
    "#     masses = np.array(masses, dtype=float)\n",
    "#     # normalize masses\n",
    "#     masses = masses / (np.sum(masses) + 1e-9)\n",
    "#     return centers, masses\n",
    "\n",
    "# --- Compute anchor centers and semantic masses ---\n",
    "def compute_anchors(Z, labels):\n",
    "    uniq = sorted(set(labels))\n",
    "    centers, masses = [], []\n",
    "    for u in uniq:\n",
    "        idxs = [i for i,l in enumerate(labels) if l == u]\n",
    "        group = Z[idxs]\n",
    "        c = group.mean(axis=0)\n",
    "        centers.append(c)\n",
    "        spread = float(np.mean(np.linalg.norm(group - c, axis=1)) + 1e-6)\n",
    "        masses.append(1.0 / spread)  # tighter -> heavier mass\n",
    "    centers = np.stack(centers, axis=0)\n",
    "    masses = np.array(masses, dtype=float)\n",
    "    masses /= (masses.sum() + 1e-9)\n",
    "    return centers, masses\n",
    "\n",
    "\n",
    "def classify_endpoint(z_final_red, centers):\n",
    "    d = np.linalg.norm(centers - z_final_red, axis=1)\n",
    "    j = int(np.argmin(d))\n",
    "    return j, float(d[j])\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Example end-to-end ----------\n",
    "def solve_with_geodesics(prompt: str,\n",
    "                         pca: PCA,\n",
    "                         anchor_centers_red: np.ndarray,\n",
    "                         masses: np.ndarray):\n",
    "    z_full = get_latent(prompt)                 # (hidden,)\n",
    "    z_red  = pca.transform(z_full[None, :])[0]  # (d,)\n",
    "\n",
    "    # Start slightly displaced with small initial velocity toward negative grad V\n",
    "    lnphi, g_lnphi = lnphi_and_grad(z_red, anchor_centers_red, masses)\n",
    "    v0 = -0.1 * g_lnphi / (np.linalg.norm(g_lnphi) + 1e-9)\n",
    "\n",
    "    traj = integrate_geodesic(z_red, v0, anchor_centers_red, masses, steps=STEPS, dt=DT)\n",
    "    z_final_red = traj[-1]\n",
    "    # Map back to full space (optional): x_full ≈ z_red * P^T + mean\n",
    "    z_full_final = pca.inverse_transform(z_final_red[None, :])[0]\n",
    "    return z_full_final, traj\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# ARC warp-interference solver\n",
    "# ----------------------------\n",
    "\n",
    "# 1) Minimal ARC transforms we’ll support right now\n",
    "def rot90_cw(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.rot90(g, k=3).tolist()\n",
    "\n",
    "def rot180(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.rot90(g, k=2).tolist()\n",
    "\n",
    "def rot270_cw(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.rot90(g, k=1).tolist()\n",
    "\n",
    "def flip_h(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.flip(g, axis=1).tolist()\n",
    "\n",
    "def flip_v(grid):\n",
    "    g = np.array(grid)\n",
    "    return np.flip(g, axis=0).tolist()\n",
    "\n",
    "# Map anchor index -> transform; adjust to your labels/anchor order\n",
    "TRANSFORM_BY_LABEL = {\n",
    "    \"rotate\": rot90_cw,       # label 0\n",
    "    \"flip_h\": flip_h,         # label 1\n",
    "    \"flip_v\": flip_v,         # label 2\n",
    "    # add more labels/anchors as you train them:\n",
    "    # \"rot180\": rot180,\n",
    "    # \"rot270\": rot270_cw,\n",
    "}\n",
    "\n",
    "# If you created anchors via sorted(set(labels)), keep the same order here:\n",
    "ANCHOR_LABELS_IN_ORDER = [\"rotate\", \"flip_h\", \"flip_v\"]  # <- keep in sync with your training labels\n",
    "\n",
    "def grid_to_prompt(grid):\n",
    "    # simple text promptization so GPT-2 latent “sees” the exact instance\n",
    "    return f\"Identify the pattern: Input grid {grid} -> Output ? (choose rotate 90° cw, flip_h, or flip_v).\"\n",
    "\n",
    "def classify_endpoint(z_final_red, centers):\n",
    "    d = np.linalg.norm(centers - z_final_red, axis=1)\n",
    "    j = int(np.argmin(d))\n",
    "    return j, float(d[j]), d\n",
    "\n",
    "def arc_latent_for_grid(grid, pca):\n",
    "    prompt = grid_to_prompt(grid)\n",
    "    z_full = get_latent(prompt)\n",
    "    z_red = pca.transform(z_full.reshape(1, -1))[0]\n",
    "    return z_red\n",
    "\n",
    "def run_warp_interference(z_red, centers, masses, steps, dt, mode=\"geodesic\"):\n",
    "    # build initial velocity along −∇lnφ (fallback to small random)\n",
    "    _, g_lnphi = lnphi_and_grad(z_red, centers, masses)\n",
    "    g_norm = float(np.linalg.norm(g_lnphi))\n",
    "    if g_norm < 1e-8:\n",
    "        rng = np.random.default_rng(0)\n",
    "        v0 = rng.normal(size=z_red.shape).astype(float)\n",
    "        v0 /= (np.linalg.norm(v0) + 1e-9)\n",
    "        v0 *= 0.05\n",
    "    else:\n",
    "        v0 = -g_lnphi / (g_norm + 1e-9) * 0.1\n",
    "\n",
    "    if mode == \"geodesic\":\n",
    "        traj = integrate_geodesic(x0=z_red, v0=v0, centers=centers, masses=masses, steps=steps, dt=dt)\n",
    "        z_final = traj[-1]\n",
    "    else:\n",
    "        # Stage-9 linearized nudge (if you kept integrate_nudge from earlier)\n",
    "        traj = integrate_nudge(z_red, centers, masses, steps=350, dt=0.05, k=2.0, gamma=0.2)\n",
    "        z_final = traj[-1]\n",
    "\n",
    "    return z_final\n",
    "\n",
    "def solve_arc_task(input_grid, *, verbose=True):\n",
    "    \"\"\"\n",
    "    input_grid: e.g. [[1,2],[3,4]] or a 3x3 integer grid.\n",
    "    Returns: predicted_output_grid (same shape as input)\n",
    "    \"\"\"\n",
    "    # 1) embed this specific instance\n",
    "    z_red = arc_latent_for_grid(input_grid, pca)\n",
    "\n",
    "    # 2) warp-interference geodesic to pick the anchor\n",
    "    z_final = run_warp_interference(z_red, anchor_centers_red, masses, steps=STEPS, dt=DT, mode=MODE)\n",
    "\n",
    "    # 3) nearest anchor = chosen transform\n",
    "    j, dmin, all_d = classify_endpoint(z_final, anchor_centers_red)\n",
    "    chosen_label = ANCHOR_LABELS_IN_ORDER[j]\n",
    "    transform = TRANSFORM_BY_LABEL.get(chosen_label, None)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[ARC] Endpoint nearest anchor: {j} ({chosen_label}) at distance {dmin:.4f}\")\n",
    "        print(f\"[ARC] Distances to anchors:\", np.array_str(all_d, precision=4, suppress_small=True))\n",
    "\n",
    "    if transform is None:\n",
    "        raise ValueError(f\"No transform mapped for anchor label '{chosen_label}'\")\n",
    "\n",
    "    # 4) apply transform to the grid\n",
    "    output_grid = transform(input_grid)\n",
    "    return output_grid\n",
    "\n",
    "def anchor_confidence(distances, tau=2.0):\n",
    "    # lower distance → higher confidence\n",
    "    s = np.exp(-(distances - distances.min()) / max(1e-9, tau))\n",
    "    p = s / (s.sum() + 1e-9)\n",
    "    j = int(np.argmin(distances))\n",
    "    return j, p[j], p\n",
    "\n",
    "\n",
    "# # ------------- Quick smoke test -------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     demo_grid = [[1,2],[3,4]]  # classic toy; true label often \"rotate\" for some tasks\n",
    "#     pred = solve_arc_task(demo_grid, verbose=True)\n",
    "#     print(\"[ARC] Predicted output grid:\", pred)\n",
    "\n",
    "#     j, dmin, all_d = classify_endpoint(z_final, anchor_centers_red)\n",
    "#     j, conf, probs = anchor_confidence(all_d, tau=2.0)\n",
    "#     print(f\"[ARC] Chosen: {ANCHOR_LABELS_IN_ORDER[j]} | dist={dmin:.4f} | conf={conf:.3f} | probs={np.array_str(probs, precision=3)}\")\n",
    "    \n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # --- minimal demo wiring (replace with your train set) ---\n",
    "#     train_prompts = [\n",
    "#         \"Identify the pattern: rotate 90° clockwise ...\",\n",
    "#         \"Identify the pattern: flip horizontally ...\",\n",
    "#         \"Identify the pattern: flip vertically ...\",\n",
    "#         # add more task archetypes...\n",
    "#     ]\n",
    "#     labels = [\"rotate\", \"flip_h\", \"flip_v\"]\n",
    "\n",
    "#     train_latents = np.stack([get_latent(p) for p in train_prompts], axis=0)\n",
    "#     # when calling pca_fit_transform for tiny sets\n",
    "#     pca, Z = pca_fit_transform(train_latents, target_var=0.99, MIN_DIMS=2, MAX_DIMS=3, verbose=True)\n",
    "\n",
    "#     if pca.n_components_ < 2:\n",
    "#         print(\"NOTE: PCA ended up 1D. Add more training prompts or lower-level pooling diversity.\")\n",
    "\n",
    "\n",
    "\n",
    "#     centers_red, masses = compute_anchors(Z, labels)\n",
    "\n",
    "#     masses = masses / (masses.sum() + 1e-9)\n",
    "#     masses *= 3.0     # try 2–5; adjust if too aggressive\n",
    "\n",
    "\n",
    "#     test_prompt = \"Identify the pattern: Input grid [[1,2],[3,4]] -> ...\"\n",
    "#     full_final, traj = solve_with_geodesics(test_prompt, pca, centers_red, masses)\n",
    "#     print(\"Reduced dims:\", pca.n_components_)\n",
    "    # print(\"Final full-latent norm:\", np.linalg.norm(full_final))\n",
    "\n",
    "    # print(\"PCA n_components:\", pca.n_components_)\n",
    "    # print(\"Explained variance (cumulative):\", np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "    # # Choose a starting prompt to evaluate/integrate from\n",
    "    # # (1) if you have a specific eval prompt:\n",
    "    # eval_prompt = \"Rotate [[2,3],[4,5]] by 90°\"  # <- replace as needed\n",
    "    # latent_full = get_latent(eval_prompt)        # shape: (D,)\n",
    "    # z_red = pca.transform(latent_full.reshape(1, -1))[0]  # shape: (pca.n_components_,)\n",
    "\n",
    "    # # build reduced anchors\n",
    "    # anchor_centers_red, masses = compute_anchors(Z, labels)\n",
    "    \n",
    "    # print(\"[Anchors]\", anchor_centers_red.shape, \"masses:\", masses)\n",
    "    \n",
    "    # # (2) or: start from one of the train examples you already embedded\n",
    "    # # idx = 0\n",
    "    # # z_red = Z[idx].copy()\n",
    "    \n",
    "    # # (3) or: start near the average anchor center (coarse test)\n",
    "    # # z_red = np.mean(anchor_centers_red, axis=0).copy()\n",
    "    \n",
    "    # # Sanity checks\n",
    "    # assert z_red.shape[-1] == anchor_centers_red.shape[-1], \\\n",
    "    #     f\"z_red dim {z_red.shape[-1]} != centers dim {anchor_centers_red.shape[-1]}\"\n",
    "    \n",
    "    # # Now these will work:\n",
    "    # lnphi, g_lnphi = lnphi_and_grad(z_red, anchor_centers_red, masses)\n",
    "    # print(\"||grad lnphi||:\", float(np.linalg.norm(g_lnphi)))\n",
    "\n",
    "    # # ---- Step: build initial velocity v0 and integrate ----\n",
    "    # # v0 along negative grad (small magnitude); fallback if grad is tiny\n",
    "    # g_norm = float(np.linalg.norm(g_lnphi))\n",
    "    # if g_norm < 1e-8:\n",
    "    #     # pick a tiny random direction to start\n",
    "    #     rng = np.random.default_rng(0)\n",
    "    #     v0 = rng.normal(size=z_red.shape).astype(float)\n",
    "    #     v0 /= (np.linalg.norm(v0) + 1e-9)\n",
    "    #     v0 *= 0.05\n",
    "    # else:\n",
    "    #     v0 = -g_lnphi / (g_norm + 1e-9) * 0.1   # step size 0.1 is a good start\n",
    "    \n",
    "    # traj = integrate_geodesic(\n",
    "    #     x0=z_red,\n",
    "    #     v0=v0,\n",
    "    #     centers=anchor_centers_red,\n",
    "    #     masses=masses,\n",
    "    #     steps=STEPS,\n",
    "    #     dt=DT\n",
    "    # )\n",
    "    \n",
    "    # # Diagnostics\n",
    "    # disp = float(np.linalg.norm(traj[-1] - traj[0]))\n",
    "    # print(\"Geodesic displacement in reduced space:\", disp)\n",
    "    \n",
    "    # # (Optional) map final point back to full latent space for downstream use\n",
    "    # z_final_red = traj[-1]\n",
    "    # z_full_final = pca.inverse_transform(z_final_red.reshape(1, -1))[0]\n",
    "    # print(\"Final full-latent norm (from geodesic end):\", float(np.linalg.norm(z_full_final)))\n",
    "\n",
    "\n",
    "    \n",
    "    # # before integrate_geodesic(...)\n",
    "    # lnphi, g_lnphi = lnphi_and_grad(z_red, anchor_centers_red, masses)\n",
    "    # print(\"||grad lnphi||:\", np.linalg.norm(g_lnphi))\n",
    "        \n",
    "    # # --- diagnostics (already have z_red, anchor_centers_red, masses from above) ---\n",
    "    # lnphi, g_lnphi = lnphi_and_grad(z_red, anchor_centers_red, masses)\n",
    "    # print(\"||grad lnphi||:\", float(np.linalg.norm(g_lnphi)))\n",
    "    \n",
    "    # # Build initial velocity v0 along −∇lnφ (fallback to small random if too tiny)\n",
    "    # g_norm = float(np.linalg.norm(g_lnphi))\n",
    "    # if g_norm < 1e-8:\n",
    "    #     rng = np.random.default_rng(0)\n",
    "    #     v0 = rng.normal(size=z_red.shape).astype(float)\n",
    "    #     v0 /= (np.linalg.norm(v0) + 1e-9)\n",
    "    #     v0 *= 0.05\n",
    "    # else:\n",
    "    #     v0 = -g_lnphi / (g_norm + 1e-9) * 0.1\n",
    "    \n",
    "    # # Integrate ONCE (remove any duplicate calls with ellipsis!)\n",
    "    # traj = integrate_geodesic(\n",
    "    #     x0=z_red,\n",
    "    #     v0=v0,\n",
    "    #     centers=anchor_centers_red,\n",
    "    #     masses=masses,\n",
    "    #     steps=STEPS,\n",
    "    #     dt=DT\n",
    "    # )\n",
    "    \n",
    "    # # Diagnostics\n",
    "    # disp = float(np.linalg.norm(traj[-1] - traj[0]))\n",
    "    # print(\"Geodesic displacement in reduced space:\", disp)\n",
    "    \n",
    "    # # Optional: lift back to full latent for downstream use\n",
    "    # z_final_red = traj[-1]\n",
    "    # z_full_final = pca.inverse_transform(z_final_red.reshape(1, -1))[0]\n",
    "    # print(\"Final full-latent norm (from geodesic end):\", float(np.linalg.norm(z_full_final)))\n",
    "\n",
    "    # j, dmin = classify_endpoint(z_final_red, anchor_centers_red)\n",
    "    # print(f\"Endpoint nearest anchor: #{j} at distance {dmin:.4f}\")\n",
    "\n",
    "    # # direction sanity: is v0 roughly -∇lnφ ?\n",
    "    # cos_theta = float(np.dot(v0, -g_lnphi) / ((np.linalg.norm(v0)+1e-9)*(np.linalg.norm(g_lnphi)+1e-9)))\n",
    "    # print(f\"cos(angle(v0, -grad lnphi)) = {cos_theta:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd77be50-769e-43b7-a28c-cf39f2283785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
