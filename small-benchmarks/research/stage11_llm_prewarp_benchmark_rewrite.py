#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
stage11_llm_prewarp_benchmark_rewrite.py

A CPU-safe, memory-tame rewrite of the "pre-warp benchmark" script.
Key changes for stability on macOS/CPU and small machines:

- No SciPy dependency (custom Gaussian smoothing via PyTorch conv2d).
- Pure NumPy SVD-based PCA with optional whitening (no scikit-learn).
- Collection runs in small batches; generation runs prompt-by-prompt.
- Forward *pre-hook* that warps the *input* hidden states of the target block.
- Safe mode disables cache, sets single-threaded BLAS, and uses eager attention.
- Optional multi-pass (refinement) to re-center after a provisional warp.
- Robust cleanup: hooks are removed even if generation fails.

Usage (example):
  python3 stage11_llm_prewarp_benchmark_rewrite.py \\
      --model gpt2 \\
      --tap -9 \\
      --calib calib_prompts.txt \\
      --eval  eval_prompts.txt \\
      --safe_mode \\
      --refine_passes 2 \\
      --alpha 0.60 --r0_frac 0.35 \\
      --out_jsonl logs/prewarp_generations.jsonl

Author: rewrite generated by ChatGPT
"""

import argparse, os, json, math, sys
from pathlib import Path
from typing import Tuple, List, Dict

import numpy as np
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM


# ----------------------------
# Utilities & Environment
# ----------------------------

def set_safe_mode():
    """Harden runtime to avoid SIGBUS/SIGSEGV on small macOS/CPU boxes."""
    # Torch hygiene
    torch.set_grad_enabled(False)
    torch.set_num_threads(1)  # tame BLAS/OMP threads on macOS
    # Env hints for MKL/OpenMP edge-cases (ignored if not present)
    os.environ.setdefault("OMP_NUM_THREADS", "1")
    os.environ.setdefault("MKL_NUM_THREADS", "1")
    os.environ.setdefault("MKL_THREADING_LAYER", "GNU")
    # Avoid experimental MPS memory spikes if present
    os.environ.setdefault("PYTORCH_ENABLE_MPS_FALLBACK", "1")
    os.environ.setdefault("PYTORCH_MPS_HIGH_WATERMARK_RATIO", "0.0")


def map_tap_to_layer_index(n_layers: int, tap: int) -> int:
    """Map negative tap to layer index. E.g., for 12 layers, tap -1 -> 11, -12 -> 0."""
    if tap >= 0:
        # Allow absolute indexing; clamp to [0, n_layers-1]
        return max(0, min(n_layers - 1, tap))
    return max(0, min(n_layers - 1, n_layers + tap))


# ----------------------------
# PCA (NumPy SVD, optional whitening)
# ----------------------------

class PCAProj:
    """Simple PCA projection using NumPy SVD (no scikit-learn). Supports whitening."""
    def __init__(self, mean: np.ndarray, components: np.ndarray, scales: np.ndarray):
        """
        mean: (D,)
        components: (k, D) rows are principal directions
        scales: (k,) scaling factors (sqrt of eigenvalues) for whitening (>= 1e-12)
        """
        self.mean = mean.astype(np.float32, copy=True)
        self.components = components.astype(np.float32, copy=True)
        self.scales = np.maximum(scales.astype(np.float32, copy=True), 1e-12)

    @staticmethod
    def fit(H: np.ndarray, k: int = 3, whiten: bool = True) -> Tuple["PCAProj", np.ndarray]:
        """
        Fit PCA on H (N,D), return (PCAProj, Y) with the first k components.
        Y is whitened if whiten=True.
        """
        H = H.astype(np.float32, copy=False)
        mean = H.mean(axis=0, dtype=np.float64).astype(np.float32)
        X = H - mean
        # Economy SVD on (N,D). Using np.linalg.svd which calls LAPACK/Accelerate.
        # For tall matrices this is stable and avoids heavy deps.
        U, S, Vt = np.linalg.svd(X, full_matrices=False)
        # Vt shape: (D, D); take top-k rows as components
        components = Vt[:k, :]
        # Eigenvalues: (S**2)/(N-1); scales for whitening = sqrt(eig) = S/sqrt(N-1)
        denom = max(1, (H.shape[0] - 1))
        eig_sqrt = S[:k] / math.sqrt(denom)
        scales = eig_sqrt if whiten else np.ones_like(eig_sqrt, dtype=np.float32)

        proj = PCAProj(mean=mean, components=components, scales=scales)
        Y = proj.transform(H)
        return proj, Y

    def transform(self, H: np.ndarray) -> np.ndarray:
        """Project to k-D (k = rows of components). Applies whitening."""
        X = (H - self.mean).astype(np.float32, copy=False)
        Y = X @ self.components.T
        Y /= self.scales  # whitening
        return Y

    def inverse_delta(self, dY: torch.Tensor) -> torch.Tensor:
        """
        Map a delta in PCA space (B, m) back to model space (B, D), accounting for whitening.
        Supports m <= k (e.g., we often warp only the first 2 PCs).
        dX = (dY * scales[:m]) @ components[:m]
        """
        device = dY.device
        m = dY.shape[-1]
        scales = torch.from_numpy(self.scales[:m]).to(device=device, dtype=dY.dtype)  # (m,)
        compsT = torch.from_numpy(self.components[:m]).to(device=device, dtype=dY.dtype)  # (m, D)
        return (dY * scales) @ compsT  # (B, D)


# ----------------------------
# Gaussian smoothing (no SciPy)
# ----------------------------

def gaussian_kernel2d(sigma: float, truncate: float = 3.0) -> torch.Tensor:
    """
    Build a normalized 2D Gaussian kernel (odd size) with given sigma.
    """
    sigma = max(1e-3, float(sigma))
    radius = int(truncate * sigma + 0.5)
    size = 2 * radius + 1
    x = torch.arange(-radius, radius + 1, dtype=torch.float32)
    xx, yy = torch.meshgrid(x, x, indexing="ij")
    g = torch.exp(-(xx*xx + yy*yy) / (2.0 * sigma * sigma))
    g /= g.sum().clamp_min(1e-12)
    return g  # (k, k)


def smooth2d(hist2d: np.ndarray, sigma_px: float) -> np.ndarray:
    """
    Smooth a 2D histogram using depthwise conv2d on CPU (Torch).
    """
    h = torch.from_numpy(hist2d.astype(np.float32, copy=False)).unsqueeze(0).unsqueeze(0)  # [1,1,H,W]
    k = gaussian_kernel2d(sigma_px)  # [K,K]
    k = k.unsqueeze(0).unsqueeze(0)  # [1,1,K,K]
    pad = k.shape[-1] // 2
    h_pad = F.pad(h, (pad, pad, pad, pad), mode="replicate")
    out = F.conv2d(h_pad, k)
    return out.squeeze(0).squeeze(0).numpy()


# ----------------------------
# Hidden-state collection
# ----------------------------

def _tokenize_batch(tok, texts: List[str], device, padding=True, truncation=True):
    enc = tok(
        texts, return_tensors="pt", padding=padding, truncation=truncation
    )
    return {k: v.to(device) for k, v in enc.items()}

def collect_hidden_states(
    model, tok, prompts: List[str], tap: int, pool: str = "lastk", k_last: int = 8,
    device: str = "cpu", batch_size: int = 8
) -> np.ndarray:
    """
    Collect pooled hidden states at 'tap' for a list of prompts in small batches.
    Returns array of shape (N, D).
    """
    assert pool in ("lastk", "mean")
    model.eval()
    outs = []
    n = len(prompts)
    with torch.inference_mode():
        for i in range(0, n, batch_size):
            chunk = prompts[i:i+batch_size]
            if not chunk:
                continue
            enc = _tokenize_batch(tok, chunk, device)
            out = model(**enc, output_hidden_states=True, use_cache=False)
            hs = out.hidden_states[tap]  # (B, T, D)
            if pool == "lastk":
                k = min(k_last, hs.shape[1])
                pooled = hs[:, -k:, :].mean(dim=1)
            else:
                pooled = hs.mean(dim=1)
            outs.append(pooled.detach().cpu().to(torch.float32))
            # Release ASAP
            del enc, out, hs, pooled
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    H = torch.cat(outs, dim=0).numpy()
    return H


# ----------------------------
# Energy map & center selection
# ----------------------------

def energy_from_eval(Y2: np.ndarray, nbins: int = 120, sigma_px: float = 5.0
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Build a smoothed density map on PCA (x,y). Return (U, Hs, xc, yc).
    U is negative density (minima = high density wells).
    """
    x, y = Y2[:, 0], Y2[:, 1]
    H, xe, ye = np.histogram2d(x, y, bins=nbins)
    Hs = smooth2d(H, sigma_px)
    U = -Hs
    xc = 0.5 * (xe[:-1] + xe[1:])
    yc = 0.5 * (ye[:-1] + ye[1:])
    return U, Hs, xc, yc


def pick_dominant_min(
    U: np.ndarray, Hs: np.ndarray, xc: np.ndarray, yc: np.ndarray,
    density_floor: float = 4.0, min_prom: float = 0.55
) -> np.ndarray:
    """
    Choose a dominant local minimum gated by density and prominence.
    Fallback to global minimum if no qualified candidate is found.
    """
    h, w = U.shape
    best = None
    best_val = np.inf
    for i in range(1, h-1):
        for j in range(1, w-1):
            if Hs[i, j] < density_floor:
                continue
            c = U[i, j]
            neigh = U[i-1:i+2, j-1:j+2].copy()
            neigh[1, 1] = np.nan
            prom = np.nanmean(neigh) - c
            if prom >= min_prom and np.all(c < np.nan_to_num(neigh, nan=np.inf)):
                if c < best_val:
                    best_val = c
                    best = (i, j)
    if best is None:
        best = np.unravel_index(np.argmin(U), U.shape)
    i, j = best
    center = np.array([xc[j], yc[i]], dtype=np.float32)
    return center


# ----------------------------
# Warp pre-hook
# ----------------------------

class WarpHook:
    """
    Context manager that registers a pre-hook on the target transformer block.
    It warps the input hidden_states in the PCA-2 plane towards a chosen center.
    """
    def __init__(self, model, layer_idx: int, pca: PCAProj,
                 center_xy: np.ndarray, r0: float, alpha: float):
        self.model = model
        self.layer_idx = int(layer_idx)
        self.pca = pca
        self.center = torch.from_numpy(center_xy.astype(np.float32))
        self.r0 = float(max(1e-6, r0))
        self.alpha = float(alpha)
        self.handle = None

    def __enter__(self):
        def pre_hook(module, inputs):
            if not isinstance(inputs, tuple) or len(inputs) == 0:
                return inputs
            H = inputs[0]
            if not torch.is_tensor(H):
                return inputs
            B, T, D = H.shape
            dev, dt = H.device, H.dtype

            # Prepare tensors on the right device/dtype
            center = self.center.to(device=dev, dtype=dt)  # (2,)
            # Project to 2D (whitened)
            X = H.reshape(-1, D)
            compsT = torch.from_numpy(self.pca.components[:2, :]).to(device=dev, dtype=dt)  # (2, D)
            mean = torch.from_numpy(self.pca.mean).to(device=dev, dtype=dt)  # (D,)
            scales = torch.from_numpy(self.pca.scales[:2]).to(device=dev, dtype=dt)  # (2,)
            Y2 = ((X - mean) @ compsT.T) / scales  # (N,2)

            # Radial contraction towards center
            V = Y2 - center
            R = torch.linalg.norm(V, dim=-1, keepdim=True).clamp_min(1e-8)
            S = 1.0 - self.alpha * torch.exp(- (R / self.r0) ** 2)
            Y2p = center + S * V
            dY2 = Y2p - Y2

            # Map delta back to model space
            dX = self.pca.inverse_delta(dY2)  # (N, D)
            Xp = X + dX
            Hp = Xp.reshape(B, T, D)

            # Replace hidden_states only; preserve other inputs
            return (Hp,) + inputs[1:]

        self.handle = self.model.transformer.h[self.layer_idx].register_forward_pre_hook(
            pre_hook, with_kwargs=False
        )
        return self

    def __exit__(self, exc_type, exc, tb):
        try:
            if self.handle is not None:
                self.handle.remove()
        finally:
            self.handle = None


# ----------------------------
# Main pipeline
# ----------------------------

def main():
    ap = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    ap.add_argument("--model", type=str, default="gpt2")
    ap.add_argument("--tap", type=int, default=-9, help="negative index into hidden_states")
    ap.add_argument("--calib", type=str, required=True, help="file: one prompt per line")
    ap.add_argument("--eval", type=str, required=True, help="file: one prompt per line")
    ap.add_argument("--pool_mode", type=str, default="lastk", choices=["lastk", "mean"])
    ap.add_argument("--k_last", type=int, default=8)
    ap.add_argument("--nbins", type=int, default=120)
    ap.add_argument("--sigma_px", type=float, default=5.0)
    ap.add_argument("--density_floor", type=float, default=4.0)
    ap.add_argument("--min_prom", type=float, default=0.55)
    ap.add_argument("--alpha", type=float, default=0.6, help="warp strength (0..1.5)")
    ap.add_argument("--r0_frac", type=float, default=0.35, help="r0 as fraction of PCA radius (0..1)")
    ap.add_argument("--refine_passes", type=int, default=1, help=">=1. If >1, do provisional warp passes to refine center.")
    ap.add_argument("--batch_size", type=int, default=8, help="batch size for hidden-state collection")
    ap.add_argument("--max_new_tokens", type=int, default=64)
    ap.add_argument("--temperature", type=float, default=0.7)
    ap.add_argument("--top_p", type=float, default=0.95)
    ap.add_argument("--out_jsonl", type=str, default="prewarp_generations.jsonl")
    ap.add_argument("--safe_mode", action="store_true",
                    help="Enable CPU/macOS safe settings and disable model cache.")
    args = ap.parse_args()

    if args.safe_mode:
        set_safe_mode()

    # Model & tokenizer
    device = "cuda" if torch.cuda.is_available() else "cpu"
    tok = AutoTokenizer.from_pretrained(args.model)
    model = AutoModelForCausalLM.from_pretrained(args.model).to(device).eval()
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token

    # Safer generation defaults
    if args.safe_mode:
        try:
            model.config.use_cache = False
        except Exception:
            pass
        try:
            model.config.attn_implementation = "eager"
        except Exception:
            pass

    # Load prompts
    with open(args.calib, "r") as f:
        calib_prompts = [ln.strip() for ln in f if ln.strip()]
    with open(args.eval, "r") as f:
        eval_prompts = [ln.strip() for ln in f if ln.strip()]
    if not calib_prompts or not eval_prompts:
        print("[ERR] Empty prompts in calib or eval.", file=sys.stderr)
        sys.exit(2)

    # 1) Collect calibration hidden states & fit PCA
    tap_idx = args.tap
    Hc = collect_hidden_states(
        model, tok, calib_prompts, tap=tap_idx, pool=args.pool_mode,
        k_last=args.k_last, device=device, batch_size=args.batch_size
    )
    pca, Yc = PCAProj.fit(Hc, k=3, whiten=True)
    r_max = float(np.linalg.norm(Yc[:, :2], axis=1).max() + 1e-8)

    # 2) Build eval energy map, pick dominant center
    He = collect_hidden_states(
        model, tok, eval_prompts, tap=tap_idx, pool=args.pool_mode,
        k_last=args.k_last, device=device, batch_size=args.batch_size
    )
    Ye = pca.transform(He)
    U, Hs, xc, yc = energy_from_eval(Ye[:, :2], nbins=args.nbins, sigma_px=args.sigma_px)
    center_xy = pick_dominant_min(U, Hs, xc, yc, density_floor=args.density_floor, min_prom=args.min_prom)

    # Optional refinement passes (provisional warp -> re-collect -> re-center)
    n_layers = len(model.transformer.h)
    layer_idx = map_tap_to_layer_index(n_layers, args.tap)
    r0 = max(1e-6, args.r0_frac * r_max)

    refine = max(1, int(args.refine_passes))
    for p in range(1, refine):
        alpha_tmp = float(args.alpha) * 0.5  # gentle provisional warp
        with WarpHook(model, layer_idx, pca, center_xy, r0, alpha_tmp):
            He2 = collect_hidden_states(
                model, tok, eval_prompts, tap=tap_idx, pool=args.pool_mode,
                k_last=args.k_last, device=device, batch_size=args.batch_size
            )
        Ye2 = pca.transform(He2)
        U2, Hs2, xc2, yc2 = energy_from_eval(Ye2[:, :2], nbins=args.nbins, sigma_px=args.sigma_px)
        center_xy = pick_dominant_min(U2, Hs2, xc2, yc2,
                                      density_floor=args.density_floor, min_prom=args.min_prom)
        # Optional: update r0 based on refined spread
        r_max = float(np.linalg.norm(Ye2[:, :2], axis=1).max() + 1e-8)
        r0 = max(1e-6, args.r0_frac * r_max)
        print(f"[REFINE {p}/{refine-1}] center={center_xy.tolist()} r0={r0:.4f}")

    print(f"[HOOK] Ready at layer index {layer_idx} for tap {args.tap}; center={center_xy.tolist()}, r0={r0:.4f}, alpha={args.alpha:.3f}")

    # 3) Generate for each prompt: stock vs pre-warp
    out_path = Path(args.out_jsonl)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with out_path.open("w", encoding="utf-8") as f:
        for ptxt in eval_prompts:
            # Stock
            enc = tok(ptxt, return_tensors="pt").to(device)
            with torch.inference_mode():
                y_stock = model.generate(
                    **enc, max_new_tokens=args.max_new_tokens,
                    do_sample=True, temperature=args.temperature, top_p=args.top_p,
                    pad_token_id=tok.eos_token_id, use_cache=False
                )
            txt_stock = tok.decode(y_stock[0], skip_special_tokens=True)

            # Geodesic (pre-warp)
            with WarpHook(model, layer_idx, pca, center_xy, r0, args.alpha):
                enc2 = tok(ptxt, return_tensors="pt").to(device)
                with torch.inference_mode():
                    y_geo = model.generate(
                        **enc2, max_new_tokens=args.max_new_tokens,
                        do_sample=True, temperature=args.temperature, top_p=args.top_p,
                        pad_token_id=tok.eos_token_id, use_cache=False
                    )
                txt_geo = tok.decode(y_geo[0], skip_special_tokens=True)

            record = {"prompt": ptxt, "stock": txt_stock, "geodesic_prewarp": txt_geo}
            f.write(json.dumps(record, ensure_ascii=False) + "\n")

            # Cleanup loop-local tensors
            del enc, y_stock, enc2, y_geo
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    print(f"[WRITE] {str(out_path)}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n[INTERRUPT] Exiting gracefully.", file=sys.stderr)
        sys.exit(130)
    except Exception as e:
        print(f"[FATAL] {type(e).__name__}: {e}", file=sys.stderr)
        sys.exit(1)
